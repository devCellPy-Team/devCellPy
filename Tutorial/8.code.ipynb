{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. CellPy Code\n",
    "\n",
    "## Required dependencies\n",
    "\n",
    "**NOTE:** Python versions for all training and all predictions must be the same. For instance, all `Layer` objects must be trained using the same Python version, and prediction of the test dataset must be the same Python version as the one in which the `Layer` objects were trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "import resource\n",
    "import sys\n",
    "import getopt\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from numpy import interp\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Ensures given files satisfy one of the possible pathways provided by CellPy\n",
    "\n",
    "Ensures user input for train or predict matches file inputs\n",
    "\n",
    "Certain files must appear together for training and/or validation to proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_combinations(user_train, user_predictOne, user_predictAll, user_fr, train_normexpr, labelinfo, train_metadata, testsplit,\n",
    "                       rejection_cutoff, val_normexpr, val_metadata, layer_paths, cardiac_dev, time_point, frsplit):\n",
    "    passed = True\n",
    "    if user_train is None:\n",
    "        print('ERROR: Run mode must be provided to resume')\n",
    "\n",
    "    # if the user selected the 'trainAll' option\n",
    "    train_list = [train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff]\n",
    "    if user_train is True:\n",
    "        print('Training option selected')\n",
    "        if train_list[0] is None:\n",
    "            print('ERROR: Normalized expression matrix for training must be provided to resume')\n",
    "            passed = False\n",
    "        if train_list[1] is None:\n",
    "            print('ERROR: Label information file must be provided to resume')\n",
    "            passed = False\n",
    "        if train_list[2] is None:\n",
    "            print('ERROR: Metadata file for training must be provided to resume')\n",
    "            passed = False\n",
    "        if train_list[3] is None:\n",
    "            print('WARNING: Test split amount not provided, training will proceed w/o cross-validation and metric calculations')\n",
    "        if train_list[4] is None:\n",
    "            print('ERROR: Rejection cutoff value must be provided to resume')\n",
    "            passed = False\n",
    "\n",
    "    # if the user selected the 'predictOne' or 'predictAll' options\n",
    "    predict_list = [val_normexpr, val_metadata, layer_paths, cardiac_dev, time_point, rejection_cutoff]\n",
    "    if (user_predictOne is True) or (user_predictAll is True):\n",
    "        if val_metadata is not None and user_predictOne is True:\n",
    "            print('Independent prediction option with accuracy calculation selected')\n",
    "        elif val_metadata is not None and user_predictAll is True:\n",
    "            print('WARNING: Dependent prediction option does not conduct accuracy calculation, provided metadata file will not be used')\n",
    "        elif val_metadata is None and user_predictOne is True:\n",
    "            print('Independent prediction option without accuracy calculation selected')\n",
    "        elif val_metadata is None and user_predictOne is True:\n",
    "            print('Dependent prediction option selected')\n",
    "        if predict_list[0] is None:\n",
    "            print('ERROR: Normalized expression matrix for prediction must be provided to resume')\n",
    "            passed = False\n",
    "        if predict_list[2] is None and predict_list[3] is False:\n",
    "            print('ERROR: Path names to Layer objects must be provided to resume')\n",
    "            passed = False\n",
    "        if predict_list[3] is True and time_point is None:\n",
    "            print('ERROR: Cardiac dev atlas timepoint must be provided to resume')\n",
    "            passed = False\n",
    "        if predict_list[5] is None:\n",
    "            print('ERROR: Rejection cutoff value must be provided to resume')\n",
    "            passed = False\n",
    "\n",
    "    # if the user selected the 'featureRankingOne' option\n",
    "    fr_list = [train_normexpr, train_metadata, layer_paths, frsplit]\n",
    "    if user_fr is True:\n",
    "        if fr_list[0] is None:\n",
    "            print('ERROR: Normalized expression matrix for training must be provided to resume')\n",
    "            passed = False\n",
    "        if fr_list[1] is None:\n",
    "            print('ERROR: Metadata file for training must be provided to resume')\n",
    "            passed = False\n",
    "        if fr_list[2] is None:\n",
    "            print('ERROR: Path names to Layer objects must be provided to resume')\n",
    "            passed = False\n",
    "        if fr_list[3] is None:\n",
    "            print('WARNING: Feature ranking split not provided but feature ranking on, will be automatically set to 0.3')\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensures all the user given variables for training exist or are in bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trainingfiles(train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff):\n",
    "    passed = True\n",
    "    if not os.path.exists(train_normexpr):\n",
    "        print('ERROR: Given normalized expression data file for training does not exist')\n",
    "        passed = False\n",
    "    if not os.path.exists(labelinfo):\n",
    "        print('ERROR: Given label info file does not exist')\n",
    "        passed = False\n",
    "    if not os.path.exists(train_metadata):\n",
    "        print('ERROR: Given metadata file for training does not exist')\n",
    "        passed = False\n",
    "    if testsplit is not None and (testsplit > 1 or testsplit < 0):\n",
    "        print('ERROR: Given test split percentage must be a value between 0 and 1')\n",
    "        passed = False\n",
    "    if rejection_cutoff > 1 or rejection_cutoff < 0:\n",
    "        print('ERROR: Given rejection cutoff must be a value between 0 and 1')\n",
    "        passed = False\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Conducts training in all layers separated into different folders by name\n",
    "\n",
    "Creates directory 'training' in cellpy_results folder, defines 'Root' as topmost layer\n",
    "\n",
    "Conducts finetuning on Root layer with 50 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff):\n",
    "    global path\n",
    "    path = os.path.join(path, 'training')\n",
    "    os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    csv2pkl(train_normexpr)\n",
    "    train_normexpr = train_normexpr[:-3] + 'pkl'\n",
    "    all_layers = [Layer('Root', 0)]\n",
    "    construct_tree(labelinfo, all_layers)\n",
    "    print(all_layers)\n",
    "    for layer in all_layers:\n",
    "        path = os.path.join(path, alphanumeric(layer.name))\n",
    "        os.mkdir(path)\n",
    "        os.chdir(path)\n",
    "        path = path + '/'\n",
    "        if layer.name == 'Root': # root top layer\n",
    "            parameters = layer.finetune(50, testsplit, train_normexpr, train_metadata)\n",
    "            print(parameters)\n",
    "        layer.train_layer(train_normexpr, train_metadata, parameters, testsplit, [0, rejection_cutoff])\n",
    "        os.chdir('..') # return to training directory\n",
    "        path = os.getcwd()\n",
    "    path = path + '/'\n",
    "    training_summary(all_layers)\n",
    "    export_layers(all_layers)\n",
    "    print('Training Complete')\n",
    "    os.chdir('..') # return to cellpy directory\n",
    "    path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts the normalized expression csv into a pkl\n",
    "\n",
    "Expression CSV file must contain genes as row names, samples as column names\n",
    "\n",
    "First column name (cell A1) is 'gene'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2pkl(csvpath):\n",
    "    tp = pd.read_csv(csvpath, iterator=True, chunksize=1000)\n",
    "    norm_express = pd.concat(tp, ignore_index=True)\n",
    "    print (norm_express.head())\n",
    "    norm_express.set_index('gene', inplace=True)\n",
    "    norm_express.index.names = [None]\n",
    "    norm_express = norm_express.T\n",
    "    norm_express.to_pickle(csvpath[:-3] + 'pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructs a list of all Layer objects from a labelinfo file\n",
    "\n",
    "Initalizes each Layer object with a name, level #, and label dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree(labelinfo, all_layers):\n",
    "    labeldata = fill_data(labelinfo)\n",
    "    fill_dict(labeldata, all_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function of construct_tree, fills the vertical columns of the labeldata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(labelinfo):\n",
    "    labeldata = list(csv.reader(open(labelinfo, encoding='utf-8-sig')))\n",
    "    # Fill in all the gaps in column 1 (layer 1)\n",
    "    for i in range(1, len(labeldata)):\n",
    "        if labeldata[i][0]=='':\n",
    "            labeldata[i][0] = labeldata[i-1][0]\n",
    "    # Fill in gaps in remaining columns only if the cell 1 left equals the value of the cell 1 up and 1 left\n",
    "    for i in range(1, len(labeldata)):\n",
    "        for j in range(1, len(labeldata[0])):\n",
    "            if labeldata[i][j]=='' and labeldata[i][j-1]==labeldata[i-1][j-1]:\n",
    "                labeldata[i][j] = labeldata[i-1][j]\n",
    "    return labeldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function of construct_tree after fill_data, reads the edited / filled labeldata file\n",
    "\n",
    "Constructs the main list structure and initializes all Layer objects in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dict(labeldata, all_layers):\n",
    "    for i in range(len(labeldata)):\n",
    "        # Initializes the Root layer dictionary with labels in the first column of labeldata\n",
    "        if find_layer(all_layers, labeldata[i][0]) is None:\n",
    "            root_layer = find_layer(all_layers, 'Root')\n",
    "            root_layer.add_dictentry(labeldata[i][0])\n",
    "        # Fills dictionaries in layers j-1 given the existence of a label in column j\n",
    "        for j in range(1,len(labeldata[0])):\n",
    "            if labeldata[i][j]!='':\n",
    "                if find_layer(all_layers, labeldata[i][j-1]) is None:\n",
    "                    all_layers.append(Layer(labeldata[i][j-1], j))\n",
    "                prev_layer = find_layer(all_layers, labeldata[i][j-1])\n",
    "                prev_layer.add_dictentry(labeldata[i][j])\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function, searches a list of all_layers for a layer with the given name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_layer(all_layers, name):\n",
    "    for layer in all_layers:\n",
    "        if layer.name == name:\n",
    "            return layer\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizes and lists the path names of all the files created during training\n",
    "\n",
    "Prints the summary of each Layer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_summary(all_layers):\n",
    "    f = open(path + 'training_summaryfile.txt', 'w')\n",
    "    for layer in all_layers:\n",
    "        f.write(str(layer))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exports all the trained Layers as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_layers(all_layers):\n",
    "    for layer in all_layers:\n",
    "        with open(path + alphanumeric(layer.name) + '_object.pkl', 'wb') as output:\n",
    "            pickle.dump(layer, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Ensures all the user given variables for predictOne or predictAll exist and are in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_predictionfiles(val_normexpr, val_metadata, layer_paths, time_point):\n",
    "    passed = True\n",
    "    if not os.path.exists(val_normexpr):\n",
    "        print('ERROR: Given validation normalized expression data file for prediction does not exist')\n",
    "        passed = False\n",
    "    if val_metadata != None and not os.path.exists(val_metadata):\n",
    "        print('ERROR: Given validation metadata file for prediction does not exist')\n",
    "        passed = False\n",
    "    # check all layer paths are objects and contain a trained xgb model\n",
    "    if time_point is None:\n",
    "        for i in range(len(layer_paths)):\n",
    "            layer_path = layer_paths[i]\n",
    "            if not os.path.exists(layer_path):\n",
    "                print('ERROR: Given Layer object ' + layer_path + ' does not exist')\n",
    "                passed = False\n",
    "            else:\n",
    "                layer = pd.read_pickle(layer_path)\n",
    "                if layer.trained() is False:\n",
    "                    print('ERROR: Given Layer object ' + layer_path + ' is not trained')\n",
    "                    passed = False\n",
    "    elif time_point > 14 or time_point < 7.5:\n",
    "        print('ERROR: Given timepoint must be a value between 7.5 and 14')\n",
    "        passed = False\n",
    "    else:\n",
    "        layer_paths = ['/CellPy-main/cardiacdevatlas_objects/Root_object.pkl',\n",
    "                       '/CellPy-main/cardiacdevatlas_objects/Cardiomyocytes_object.pkl',\n",
    "                       '/CellPy-main/cardiacdevatlas_objects/E7.75_object.pkl',\n",
    "                       '/CellPy-main/cardiacdevatlas_objects/E8.25_object.pkl',\n",
    "                       '/CellPy-main/cardiacdevatlas_objects/E9.25_object.pkl',\n",
    "                       '/CellPy-main/cardiacdevatlas_objects/E10.5_object.pkl',\n",
    "                       '/CellPy-main/cardiacdevatlas_objects/E13.5_object.pkl',\n",
    "                       '/CellPy-main/cardiacdevatlas_objects/featurenames.csv']\n",
    "        for i in range(len(layer_paths)-1):\n",
    "            layer_path = layer_paths[i]\n",
    "            if not os.path.exists(path_cda + layer_path):\n",
    "                print('ERROR: Current directory ' + path_cda + ' does not contain cardiac dev atlas object ' + layer_path)\n",
    "                passed = False\n",
    "        featurenames_path = layer_paths[len(layer_paths)-1]\n",
    "        if not os.path.exists(path_cda + featurenames_path):\n",
    "            print('ERROR: Current directory ' + path_cda + ' does not contain list of cardiac dev atlas feature names ' + featurenames_path)\n",
    "            passed = False\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducts prediction in specified layers separated into different folders by name\n",
    "\n",
    "Creates directory 'predictionOne' in cellpy_results folder, defines 'Root' as topmost layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionOne(val_normexpr, val_metadata, object_paths):\n",
    "    global path\n",
    "    path = os.path.join(path, 'predictionOne')\n",
    "    os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    all_layers = import_layers(object_paths)\n",
    "    featurenames = all_layers[0].xgbmodel.feature_names\n",
    "    reorder_pickle(val_normexpr, featurenames)\n",
    "    val_normexpr = val_normexpr[:-3] + 'pkl'\n",
    "    for layer in all_layers:\n",
    "        path = os.path.join(path, alphanumeric(layer.name))\n",
    "        os.mkdir(path)\n",
    "        os.chdir(path)\n",
    "        path = path + '/'\n",
    "        layer.predict_layer([0, rejection_cutoff], val_normexpr, val_metadata)\n",
    "        os.chdir('..') # return to prediction directory\n",
    "        path = os.getcwd()\n",
    "    print('Prediction Complete')\n",
    "    os.chdir('..') # return to cellpy directory\n",
    "    path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducts prediction in all layers in one folder\n",
    "\n",
    "Creates directory 'predictionAll' in cellpy_results folder, defines 'Root' as topmost layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionAll(val_normexpr, object_paths):\n",
    "    global path\n",
    "    path = os.path.join(path, 'predictionAll')\n",
    "    os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    path = path + '/'\n",
    "\n",
    "    all_layers = import_layers(object_paths)\n",
    "    print(all_layers)\n",
    "    featurenames = all_layers[0].xgbmodel.feature_names\n",
    "    reorder_pickle(val_normexpr, featurenames)\n",
    "    val_normexpr = val_normexpr[:-3] + 'pkl'\n",
    "\n",
    "    norm_express = pd.read_pickle(val_normexpr)\n",
    "    feature_names = list(norm_express)\n",
    "    print(norm_express.shape)\n",
    "    X = norm_express.values\n",
    "\n",
    "    X = norm_express.values\n",
    "    norm_express.index.name = 'cells'\n",
    "    norm_express.reset_index(inplace=True)\n",
    "    Y = norm_express.values\n",
    "    all_cellnames = Y[:,0]\n",
    "    all_cellnames = all_cellnames.ravel()\n",
    "    Y = None\n",
    "\n",
    "    f = open(path + 'predictionall_reject' + str(rejection_cutoff) + '.csv','w')\n",
    "    for i in range(len(all_cellnames)):\n",
    "        sample = np.array(X[i])#.reshape((-1,1))\n",
    "        sample = np.vstack((sample, np.zeros(len(feature_names))))\n",
    "        d_test = xgb.DMatrix(sample, feature_names=feature_names)\n",
    "        root_layer = find_layer(all_layers, 'Root')\n",
    "        root_layer.add_dictentry('Unclassified')\n",
    "        probabilities_xgb = root_layer.xgbmodel.predict(d_test)\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        if probabilities_xgb[0,probabilities_xgb.argmax(axis=1)[0]] < rejection_cutoff:\n",
    "            predictions_xgb[0] = len(root_layer.labeldict)-1\n",
    "        f.write(all_cellnames[i])\n",
    "        f.write(',')\n",
    "        f.write(root_layer.labeldict[predictions_xgb[0]])\n",
    "\n",
    "        search_str = root_layer.labeldict[predictions_xgb[0]]\n",
    "        del root_layer.labeldict[len(root_layer.labeldict)-1]\n",
    "        while(True):\n",
    "            curr_layer = find_layer(all_layers, search_str)\n",
    "            if curr_layer is not None:\n",
    "                curr_layer.add_dictentry('Unclassified')\n",
    "                probabilities_xgb = curr_layer.xgbmodel.predict(d_test)\n",
    "                predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "                if probabilities_xgb[0,probabilities_xgb.argmax(axis=1)[0]] < rejection_cutoff:\n",
    "                    predictions_xgb[0] = len(curr_layer.labeldict)-1\n",
    "                f.write(',')\n",
    "                f.write(curr_layer.labeldict[predictions_xgb[0]])\n",
    "                search_str = curr_layer.labeldict[predictions_xgb[0]]\n",
    "                del curr_layer.labeldict[len(curr_layer.labeldict)-1]\n",
    "            else:\n",
    "                break\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "    print('Prediction Complete')\n",
    "    os.chdir('..') # return to cellpy directory\n",
    "    path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cardiac Developmental Atlas**\n",
    "\n",
    "Conducts prediction in specified layers separated into different folders by name for the cardiac dev atlas\n",
    "\n",
    "Creates directory 'predictionOne' in cellpy_results folder, defines 'Root' as topmost layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionOneCDA(val_normexpr, val_metadata, object_paths):\n",
    "    global path\n",
    "    path = os.path.join(path, 'predictionOne')\n",
    "    os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    all_layers = import_layers(object_paths)\n",
    "    featurenames = open(path_cda + '/CellPy-main/cardiacdevatlas_objects/featurenames.csv', 'rt').read().split(',')\n",
    "    reorder_pickle(val_normexpr, featurenames)\n",
    "    val_normexpr = val_normexpr[:-3] + 'pkl'\n",
    "    for layer in all_layers:\n",
    "        path = os.path.join(path, alphanumeric(layer.name))\n",
    "        os.mkdir(path)\n",
    "        os.chdir(path)\n",
    "        path = path + '/'\n",
    "        layer.predict_layer([0, rejection_cutoff], val_normexpr, val_metadata)\n",
    "        os.chdir('..') # return to prediction directory\n",
    "        path = os.getcwd()\n",
    "    print('Prediction Complete')\n",
    "    os.chdir('..') # return to cellpy directory\n",
    "    path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cardiac Developmental Atlas**\n",
    "\n",
    "Conducts prediction in all layers in one folder for the cardiac dev atlas\n",
    "\n",
    "Creates directory 'predictionAll' in cellpy_results folder, defines 'Root' as topmost layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionAllCDA(val_normexpr, object_paths):\n",
    "    global path\n",
    "    path = os.path.join(path, 'predictionAll')\n",
    "    os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    path = path + '/'\n",
    "\n",
    "    all_layers = import_layers(object_paths)\n",
    "    print(all_layers)\n",
    "    featurenames = open(path_cda + '/CellPy-main/cardiacdevatlas_objects/featurenames.csv', 'rt').read().split(',')\n",
    "    reorder_pickle(val_normexpr, featurenames)\n",
    "    val_normexpr = val_normexpr[:-3] + 'pkl'\n",
    "\n",
    "    norm_express = pd.read_pickle(val_normexpr)\n",
    "    feature_names = list(norm_express)\n",
    "    print(norm_express.shape)\n",
    "    X = norm_express.values\n",
    "\n",
    "    X = norm_express.values\n",
    "    norm_express.index.name = 'cells'\n",
    "    norm_express.reset_index(inplace=True)\n",
    "    Y = norm_express.values\n",
    "    all_cellnames = Y[:,0]\n",
    "    all_cellnames = all_cellnames.ravel()\n",
    "    Y = None\n",
    "\n",
    "    f = open(path + 'predictionall_reject' + str(rejection_cutoff) + '.csv','w')\n",
    "    for i in range(len(all_cellnames)):\n",
    "        sample = np.array(X[i])#.reshape((-1,1))\n",
    "        sample = np.vstack((sample, np.zeros(len(feature_names))))\n",
    "        d_test = xgb.DMatrix(sample, feature_names=feature_names)\n",
    "        root_layer = all_layers[0]\n",
    "        root_layer.add_dictentry('Unclassified')\n",
    "        probabilities_xgb = root_layer.xgbmodel.predict(d_test)\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        if probabilities_xgb[0,probabilities_xgb.argmax(axis=1)[0]] < rejection_cutoff:\n",
    "            predictions_xgb[0] = len(root_layer.labeldict)-1\n",
    "        f.write(all_cellnames[i])\n",
    "        f.write(',')\n",
    "        f.write(root_layer.labeldict[predictions_xgb[0]])\n",
    "        search_str = root_layer.labeldict[predictions_xgb[0]]\n",
    "        del root_layer.labeldict[len(root_layer.labeldict)-1]\n",
    "\n",
    "        if search_str == 'Cardiomyocytes':\n",
    "            cm_layer = all_layers[1]\n",
    "            cm_layer.add_dictentry('Unclassified')\n",
    "            probabilities_xgb = cm_layer.xgbmodel.predict(d_test)\n",
    "            predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "            if probabilities_xgb[0,probabilities_xgb.argmax(axis=1)[0]] < rejection_cutoff:\n",
    "                predictions_xgb[0] = len(cm_layer.labeldict)-1\n",
    "            f.write(',')\n",
    "            f.write(cm_layer.labeldict[predictions_xgb[0]])\n",
    "            search_str = cm_layer.labeldict[predictions_xgb[0]]\n",
    "            del cm_layer.labeldict[len(cm_layer.labeldict)-1]\n",
    "\n",
    "            if search_str == 'Ventricular CM':\n",
    "                vent_layer = all_layers[2]\n",
    "                vent_layer.add_dictentry('Unclassified')\n",
    "                probabilities_xgb = vent_layer.xgbmodel.predict(d_test)\n",
    "                predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "                if probabilities_xgb[0,probabilities_xgb.argmax(axis=1)[0]] < rejection_cutoff:\n",
    "                    predictions_xgb[0] = len(vent_layer.labeldict)-1\n",
    "                f.write(',')\n",
    "                f.write(vent_layer.labeldict[predictions_xgb[0]])\n",
    "                search_str = vent_layer.labeldict[predictions_xgb[0]]\n",
    "                del vent_layer.labeldict[len(vent_layer.labeldict)-1]\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "    print('Prediction Complete')\n",
    "    os.chdir('..') # return to cellpy directory\n",
    "    path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports Layer objects from a list of given paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_layers(layer_paths):\n",
    "    layers = []\n",
    "    for layer_path in layer_paths:\n",
    "         layer = pd.read_pickle(layer_path)\n",
    "         layers.append(layer)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts the normalized expression csv into a pkl\n",
    "\n",
    "Expression CSV file must contain genes as row names, samples as column names\n",
    "\n",
    "First column name (cell A1) is 'gene'\n",
    "\n",
    "Reorders the csv file to match the features in a given featurenames list\n",
    "\n",
    "Returns path to the new pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef reorder_pickle(csvpath, featurenames):\n",
    "    tp = pd.read_csv(csvpath, iterator=True, chunksize=1000)\n",
    "    norm_express = pd.concat(tp, ignore_index=True)\n",
    "    print (norm_express.head())\n",
    "    norm_express.set_index('gene', inplace=True)\n",
    "    norm_express.index.names = [None]\n",
    "    norm_express = norm_express.T\n",
    "    print(norm_express.T.duplicated().any())\n",
    "    print ('Training Data # of  genes: ' + str(len(featurenames)))\n",
    "\n",
    "    ## Manually reorder columns according to training data index\n",
    "    # Reorder overlapping genes, remove genes not in training data\n",
    "    origfeat = list(norm_express)\n",
    "    print ('Validation Data # of genes: ' + str(len(origfeat)))\n",
    "    newindex = []\n",
    "    for i in range(len(featurenames)):\n",
    "        if featurenames[i] in origfeat:\n",
    "            newindex.append(featurenames[i])\n",
    "    print ('Overlapping # of genes: ' + str(len(newindex)))\n",
    "    norm_express = norm_express.reindex(columns=newindex)\n",
    "    # Add missing features, remove extra features to match atlas\n",
    "    i = 0\n",
    "    missing_counter = 0\n",
    "    while i < len(list(norm_express)):\n",
    "        if list(norm_express)[i] != featurenames[i]:\n",
    "            norm_express.insert(i, featurenames[i], None)\n",
    "            missing_counter += 1\n",
    "        i += 1\n",
    "    while i < len(featurenames):\n",
    "        norm_express.insert(i, featurenames[i], None)\n",
    "        i += 1\n",
    "        missing_counter += 1\n",
    "    # Overlapping + missing = training total\n",
    "    print ('Missing # of genes: ' + str(missing_counter))\n",
    "    norm_express.to_pickle(csvpath[:-3] + 'pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensures all the user given variables for training exist or are in bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_featurerankingfiles(train_normexpr, train_metadata, layer_paths, frsplit):\n",
    "    passed = True\n",
    "    if not os.path.exists(train_normexpr):\n",
    "        print('ERROR: Given normalized expression data file for training does not exist')\n",
    "        passed = False\n",
    "    if not os.path.exists(train_metadata):\n",
    "        print('ERROR: Given metadata file for training does not exist')\n",
    "        passed = False\n",
    "    # check all layer paths are objects and contain a trained xgb model\n",
    "    if layer_paths != None:\n",
    "        for i in range(len(layer_paths)):\n",
    "            layer_path = layer_paths[i]\n",
    "            if not os.path.exists(layer_path):\n",
    "                print('ERROR: Given Layer object ' + str(i) + ' does not exist')\n",
    "                passed = False\n",
    "            else:\n",
    "                layer = pd.read_pickle(layer_path)\n",
    "                if layer.trained() is False:\n",
    "                    print('ERROR: Given Layer object ' + str(i) + ' is not trained')\n",
    "                    passed = False\n",
    "    if frsplit is not None and (frsplit > 1 or frsplit < 0):\n",
    "        print('ERROR: Given feature ranking split must be a value between 0 and 1')\n",
    "        passed = False\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducts prediction in all layers separated into different folders by name\n",
    "\n",
    "Creates directory 'featureranking' in cellpy_results folder, defines 'Root' as topmost layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureranking(train_normexpr, train_metadata, object_paths, frsplit):\n",
    "    global path\n",
    "    path = os.path.join(path, 'featureranking')\n",
    "    os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    csv2pkl(train_normexpr)\n",
    "    train_normexpr = train_normexpr[:-3] + 'pkl'\n",
    "    all_layers = import_layers(object_paths)\n",
    "    for layer in all_layers:\n",
    "        path = os.path.join(path, alphanumeric(layer.name))\n",
    "        os.mkdir(path)\n",
    "        os.chdir(path)\n",
    "        path = path + '/'\n",
    "        layer.featurerank_layer(train_normexpr, train_metadata, frsplit)\n",
    "        os.chdir('..') # return to prediction directory\n",
    "        path = os.getcwd()\n",
    "    print('Feature Ranking Complete')\n",
    "    os.chdir('..') # return to cellpy directory\n",
    "    path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphanumeric(str):\n",
    "    temparr = list([val for val in str if val.isalpha() or val.isnumeric()])\n",
    "    cleanstr = ''.join(temparr)\n",
    "    return cleanstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Class\n",
    "\n",
    "Object for each layer of the model\n",
    "\n",
    "Contains methods for subsetting data according to the metadata and label info files,\n",
    "                     - dividing subsetted data 90-10 and 10 fold cv,\n",
    "                     - finetuning and training each layer, calculating metrics and SHAP feature ranking,\n",
    "                     - outputting final model for validation\n",
    "                     \n",
    "Keeps track of all outputted files, stores paths and names in instance variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    # level is the column of the metadata file in which the name of the layer appears\n",
    "    # xgbmodel is a trained XGBoost classifier object\n",
    "    # cvmetrics, finalmetrics, cfm, pr are path names to those files\n",
    "    # predictions, roc, fr are lists of path names\n",
    "    def __init__(self, name, level, labeldict=None, xgbmodel=None, finetuning=None, cvmetrics=None,\n",
    "                 finalmetrics=None, predictions=None, cfm=None, roc=None, pr=None, pickle=None):\n",
    "        self.name = name\n",
    "        self.level = level\n",
    "        self.labeldict = {}\n",
    "        self.xgbmodel = None\n",
    "        self.finetuning = None\n",
    "        self.cvmetrics = None\n",
    "        self.finalmetrics = None\n",
    "        self.predictions = []\n",
    "        self.cfm = None\n",
    "        self.roc = []\n",
    "        self.pr = None\n",
    "        self.pickle = self.name + '_object.pkl'\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __eq__(self, layer2):\n",
    "        return self.name==layer2.name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<Layer: '%s', Level: %s, labeldict: %s, Trained: %s>\" % (self.name, self.level, self.labeldict, self.trained())\n",
    "\n",
    "    def __str__(self):\n",
    "        return_str = 'Object: ' + self.pickle + '\\n'\n",
    "        return_str += 'Layer: ' + self.name + '\\n'\n",
    "        return_str += 'Level: ' + str(self.level) + '\\n'\n",
    "        return_str += 'Label Dictionary: ' + str(self.labeldict) + '\\n'\n",
    "        if self.finetuned():\n",
    "            return_str += 'Finetuning Log: ' + self.finetuning + '\\n'\n",
    "        if self.trained():\n",
    "            return_str += 'XGB Model: ' + self.name + '_xgbmodel.sav' + '\\n'\n",
    "            if self.cvmetrics is not None:\n",
    "                return_str += '10-Fold Cross Validation Metrics: ' + self.cvmetrics + '\\n'\n",
    "                return_str += 'Final Metrics: ' + self.finalmetrics + '\\n'\n",
    "                return_str += 'Predictions (no rejection): ' + self.predictions[0] + '\\n'\n",
    "                return_str += 'Predictions (' + str(rejection_cutoff) + ' rejection cutoff): ' + self.predictions[1] + '\\n'\n",
    "                return_str += 'Confusion Matrix (w/ rejection): ' + self.cfm + '\\n'\n",
    "                return_str += 'Micro/Macro ROC Curves: ' + self.roc[0] + '\\n'\n",
    "                return_str += 'Per-Class ROC Curves: ' + self.roc[1] + '\\n'\n",
    "                return_str += 'All Combined ROC Curves: ' + self.roc[2] + '\\n'\n",
    "                return_str += 'Precision-Recall Curves: ' + self.pr + '\\n'\n",
    "        return return_str\n",
    "\n",
    "    def finetuned(self):\n",
    "        return self.finetuning is not None\n",
    "\n",
    "    def trained(self):\n",
    "        return self.xgbmodel is not None\n",
    "\n",
    "    # Adds a value to the label dictionary if it is not already present\n",
    "    # Utility function of fill_dict\n",
    "    def add_dictentry(self, value):\n",
    "        if value not in list(self.labeldict.values()):\n",
    "            self.labeldict[len(self.labeldict)] = value\n",
    "\n",
    "\n",
    "    # Hyperparameter tuning for XGBoost using accuracy\n",
    "    # Equivalent to sklearn.model_selection.RandomizedSearchCV -- w/o cv and w/ triangular dist\n",
    "    # Parameters: eta, max_depth, subsamaple, colsample_bytree\n",
    "    # n randomized trials: triangular distribution around the default or recommended xgboost value\n",
    "    #                      rounded to a reasonable decimal place\n",
    "    # Splits data according to user-provided testplit value, NO cross validation conducted\n",
    "    # Returns parameters with lowest mean absolute error (mae) on 10% for actual training\n",
    "    # Outputs csv file with accuracy of each class and mae for all trials, does not output any other metrics\n",
    "    def finetune(self, trials, testsplit, normexprpkl, metadatacsv):\n",
    "        # If user skips cross validation, testsplit automatically set to 10% for finetuning\n",
    "        if testsplit is None:\n",
    "            testsplit = 0.1\n",
    "        X, Y, X_tr, X_test, Y_tr, Y_test, _ = self.read_data(normexprpkl, metadatacsv, testsplit)\n",
    "        min_mae = 100000000000000\n",
    "        f = open(path + self.name + '_finetuning.csv', 'a+')\n",
    "        f.write('ETA,Max Depth,Subsample,Colsample by Tree,')\n",
    "        for i in range(len(self.labeldict)):\n",
    "            f.write(self.labeldict[i] + ' Accuracy')\n",
    "            f.write(',')\n",
    "        f.write('MAE\\n')\n",
    "        for i in range(trials):\n",
    "            eta_temp = round(random.triangular(0,1,0.3),1)\n",
    "            max_depth_temp = round(random.triangular(4,8,6))\n",
    "            subsample_temp = round(random.triangular(0.01,1,0.5),1)\n",
    "            colsample_bytree_temp = round(random.triangular(0.01,1,0.5),1)\n",
    "            params = {'objective': 'multi:softprob', 'eta': eta_temp, 'max_depth': max_depth_temp, 'subsample': subsample_temp,\n",
    "                    'colsample_bytree': colsample_bytree_temp, 'eval_metric': 'merror', 'seed': 840}\n",
    "\n",
    "            mae, output_string = self.xgboost_model_shortver(X_tr, X_test, Y_tr, Y_test, params)\n",
    "            f.write(str(eta_temp) + ',' + str(max_depth_temp) + ',' + str(subsample_temp) + ',' + str(colsample_bytree_temp) + ',')\n",
    "            f.write(output_string + '\\n')\n",
    "            if mae < min_mae:\n",
    "                min_mae = mae\n",
    "                final_params = params\n",
    "        print(final_params)\n",
    "        self.finetuning = self.name + '_finetuning.csv'\n",
    "        return final_params\n",
    "\n",
    "    # XGBoost w/o cross validation, no output files, just accuracy score on user-provided testsplit value\n",
    "    # Returns mean absolute error (mae) as float and csv output string\n",
    "    #       Accuracy for each class and mae in a comma-separated string for classification\n",
    "    # Only for finetuning!\n",
    "    def xgboost_model_shortver(self, X_tr, X_test, Y_tr, Y_test, params):\n",
    "        params['num_class'] = len(self.labeldict)\n",
    "\n",
    "        d_tr = xgb.DMatrix(X_tr, Y_tr, feature_names=feature_names)\n",
    "        model = xgb.train(params, d_tr, 20, verbose_eval=10)\n",
    "        d_test = xgb.DMatrix(X_test, Y_test, feature_names=feature_names)\n",
    "        probabilities_xgb = model.predict(d_test)\n",
    "\n",
    "        returned_str = ''\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        cm = confusion_matrix(Y_test, predictions_xgb)\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        mae = 1-sum(cm.diagonal())/len(cm.diagonal())\n",
    "        returned_str = ','.join(map(str, cm.diagonal())) + ',' + str(mae)\n",
    "        return mae, returned_str\n",
    "\n",
    "\n",
    "    # Trains one layer in the classification\n",
    "    # Trains XGBoost for a layer of classification given parameters\n",
    "    # Splits data according to user-provided testsplit\n",
    "    # Conducts 10-fold cross validation on (1-testsplit)% of data, outputs cv metrics\n",
    "    # Retrains model on (1-testsplit)% to output metrics when tested on holdout (testsplit)%\n",
    "    # Retrains final saved model on 100% of data, returns for feature ranking\n",
    "    # Conducts feature ranking with SHAP if instructed by user on full final model\n",
    "    def train_layer(self, normexprpkl, metadatacsv, params, testsplit, rejectcutoffs):\n",
    "        params['num_class'] = len(self.labeldict)\n",
    "\n",
    "        if testsplit is not None:\n",
    "            # 10-fold CV on (1-testsplit)% of data\n",
    "            X, Y, X_tr, X_test, Y_tr, Y_test, test_cellnames = self.read_data(normexprpkl, metadatacsv, testsplit)\n",
    "            kfold = 10\n",
    "            sss = StratifiedShuffleSplit(n_splits=kfold, test_size=0.1, random_state=720)\n",
    "            for i, (train_index, test_index) in enumerate(sss.split(X_tr, Y_tr)):\n",
    "                print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "                X_train, X_valid = X_tr[train_index], X_tr[test_index]\n",
    "                Y_train, Y_valid = Y_tr[train_index], Y_tr[test_index]\n",
    "                d_train = xgb.DMatrix(X_train, Y_train, feature_names=feature_names)\n",
    "                cv_model = xgb.train(params, d_train, 20, verbose_eval=500)\n",
    "                self.xgbmodel = cv_model # temporarily set xgbmodel to cv 0.9*(1-testsplit)% model\n",
    "                self.model_metrics('cv', rejectcutoffs[1], X_valid, Y_valid)\n",
    "                self.cvmetrics = self.name + '_cvmetrics.txt'\n",
    "\n",
    "            # 90% model, 10% testing\n",
    "            d_tr = xgb.DMatrix(X_tr, Y_tr, feature_names=feature_names)\n",
    "            temp_model = xgb.train(params, d_tr, 20, verbose_eval=500)\n",
    "            self.xgbmodel = temp_model # temporarily set xgbmodel to (1-testsplit)% model\n",
    "            self.model_metrics('final', rejectcutoffs[1], X_test, Y_test)\n",
    "            self.finalmetrics = self.name + '_finalmetrics.txt'\n",
    "            self.cell_predictions('training', rejectcutoffs, test_cellnames, X_test, Y_test)\n",
    "            for cutoff in rejectcutoffs:\n",
    "                self.predictions.append(self.name + '_trainingpredictions_reject' + str(cutoff) + '.csv')\n",
    "            self.cfsn_mtx('training', rejectcutoffs[1], X_test, Y_test)\n",
    "            self.cfm = self.name + '_confusionmatrix.svg'\n",
    "            self.roc_curves(X_test, Y_test)\n",
    "            self.roc.append(self.name + '_miacroroc.svg')\n",
    "            self.roc.append(self.name + '_classroc.svg')\n",
    "            self.roc.append(self.name + '_allroc.svg')\n",
    "            self.pr_curves(X_test, Y_test)\n",
    "            self.pr = self.name + '_allpr.svg'\n",
    "\n",
    "        # skip cross validation and metric calculations\n",
    "        if testsplit is None:\n",
    "            X, Y, all_cellnames = self.read_data(normexprpkl, metadatacsv)\n",
    "\n",
    "        # final 100% model\n",
    "        d_all = xgb.DMatrix(X, Y, feature_names=feature_names)\n",
    "        final_model = xgb.train(params, d_all, 20, verbose_eval=500)\n",
    "        pickle.dump(final_model, open(path + self.name + '_xgbmodel.sav', 'wb'))\n",
    "        self.xgbmodel = final_model\n",
    "\n",
    "\n",
    "    # Outputs predictions of the 90% model on the 10% test set in a given dataset\n",
    "    # 1 csv file outputted for each rejection cutoff in the list rejectioncutoffs\n",
    "    # A cutoff of 0.5 means the probability of the label must be >=0.5 for a prediction to be called\n",
    "    # A cutoff of 0 is equivalent to no rejection option\n",
    "    def predict_layer(self, rejectcutoffs, normexprpkl, metadatacsv=None):\n",
    "        X, Y, all_cellnames = self.read_data(normexprpkl, metadatacsv) # Y will be None if metadatacsv=None\n",
    "        self.cell_predictions('validation', rejectcutoffs, all_cellnames, X, Y) # can handle Y=None\n",
    "        if metadatacsv != None:\n",
    "            self.cfsn_mtx('validation', rejectcutoffs[1], X, Y)\n",
    "            self.model_metrics('validation', rejectcutoffs[1], X, Y)\n",
    "\n",
    "\n",
    "    # Outputs SHAP feature ranking plots overall and for each class\n",
    "    # Outputs csv table containing overall SHAP values\n",
    "    # Uses same procedure for subsetting norm_expr as read_data(...) if column & criterium are provided\n",
    "    # frsplit automatically set to 0.3 if not provided\n",
    "    def featurerank_layer(self, normexprpkl, metadatacsv, frsplit):\n",
    "        norm_express = pd.read_pickle(normexprpkl)\n",
    "        tp = pd.read_csv(metadatacsv, iterator=True, chunksize=1000)\n",
    "        labels = pd.concat(tp, ignore_index=True)\n",
    "        labels.set_index('Unnamed: 0', inplace=True)\n",
    "        labels.index.names = [None]\n",
    "        # Only root level will have no subsetcolumn, all other levels require subsetting\n",
    "        # self.level is one less than what user sees since cellname column is dropped above\n",
    "        metadata_columns = list(labels.columns.values)\n",
    "        labelcolumn = metadata_columns[self.level]\n",
    "        subsetcolumn = metadata_columns[self.level-1] if self.level > 0 else None\n",
    "        subsetcriterium = self.name\n",
    "        # Filter out cells if subsetting necessary, keep only data from given criterium\n",
    "        if subsetcolumn != None:\n",
    "            # Reindex norm_express and labels based on cell names in given criterium\n",
    "            temp = labels.loc[labels[subsetcolumn] == subsetcriterium]\n",
    "            labels = labels.reindex(index=temp.index)\n",
    "            norm_express = norm_express.reindex(index=temp.index)\n",
    "        # Remove cells with labels not provided in the dictionary\n",
    "        temp = labels.loc[labels[labelcolumn].isin(list(self.labeldict.values()))]\n",
    "        labels = labels.reindex(index=temp.index)\n",
    "        norm_express = norm_express.reindex(index=temp.index)\n",
    "        norm_express, labels = shuffle(norm_express, labels)\n",
    "        if frsplit is None:\n",
    "            frsplit = 0.3\n",
    "        if frsplit != 1:\n",
    "            norm_express = train_test_split(norm_express, labels, test_size=frsplit,\n",
    "                                            random_state=42, shuffle = True, stratify = labels[labelcolumn])[1]\n",
    "\n",
    "        model = self.xgbmodel\n",
    "        model_bytearray = model.save_raw()[4:]\n",
    "        model.save_raw = lambda: model_bytearray\n",
    "        shap_values = shap.TreeExplainer(model).shap_values(norm_express)\n",
    "        shap.summary_plot(shap_values, norm_express, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path + self.name + '_overallfr.svg')\n",
    "        plt.clf()\n",
    "        for i in range(len(self.labeldict)):\n",
    "            shap.summary_plot(shap_values[i], norm_express, show=False)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(path + self.name + '_' + alphanumeric(self.labeldict[i]) + 'fr.svg')\n",
    "            #np.savetxt(path + name + '_class'+str(i)+'fr.csv', shap_values[i], delimiter=\",\")\n",
    "            plt.clf()\n",
    "\n",
    "        vals = np.abs(shap_values).mean(0)\n",
    "        feature_importance = pd.DataFrame(list(zip(norm_express.columns, sum(vals))), columns=['Gene','Feature Importance Value'])\n",
    "        feature_importance.sort_values(by=['Feature Importance Value'], ascending=False, inplace=True)\n",
    "        feature_importance.to_csv(path + self.name + '_featureimportances.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "        print('Overall Feature Ranking: ' + self.name + '_overallfr.svg')\n",
    "        for i in range(len(self.labeldict)):\n",
    "            print(self.name + '--' + list(self.labeldict.values())[i] + ' Feature Ranking: ' + self.name + '_' + alphanumeric(self.labeldict[i]) + 'fr.svg')\n",
    "        print('Full Feature Importance List: ' + self.name + '_featureimportances.csv')\n",
    "\n",
    "\n",
    "    # Reads the normalized gene expression data into a norm_expr dataframe\n",
    "    # Reads the specified column in the metadata csv into a label dataframe\n",
    "    # Metadata CSV file must contain samples as row names, first column name (cell A1) is empty\n",
    "    # subsetcolumn and subsetcriterium are strings for extracting rows with the specified label in the specified column\n",
    "    #                                 eg. for per timepoint classification\n",
    "    #                                 can both be set to None if all samples are to be included\n",
    "    # Converts norm_expr and label dataframes into numpy arrays X and Y, splits them into 90/10\n",
    "    # Retrieves cell names of samples in 10% testing data\n",
    "    # global feature_names defined here, list of all gene names -> should be equivalent for all csvs (except subsetted versions)\n",
    "    def read_data(self, normexprpkl, metadatacsv=None, testsplit=None):\n",
    "        norm_express = pd.read_pickle(normexprpkl)\n",
    "        global feature_names\n",
    "        feature_names = list(norm_express)\n",
    "        print(norm_express.shape)\n",
    "\n",
    "        # If validation w/o metadata, metadata not provided, return all data w/o subsetting\n",
    "        if metadatacsv is None:\n",
    "            X = norm_express.values\n",
    "            norm_express.index.name = 'cells'\n",
    "            norm_express.reset_index(inplace=True)\n",
    "            Y = norm_express.values\n",
    "            all_cellnames = Y[:,0]\n",
    "            all_cellnames = all_cellnames.ravel()\n",
    "            Y = None\n",
    "            return X, Y, all_cellnames\n",
    "\n",
    "        tp = pd.read_csv(metadatacsv, iterator=True, chunksize=1000)\n",
    "        labels = pd.concat(tp, ignore_index=True)\n",
    "        labels.set_index('Unnamed: 0', inplace=True)\n",
    "        labels.index.names = [None]\n",
    "\n",
    "        # Only root level will have no subsetcolumn, all other levels require subsetting\n",
    "        # self.level is one less than what user sees since index is dropped above\n",
    "        metadata_columns = list(labels.columns.values)\n",
    "        labelcolumn = metadata_columns[self.level]\n",
    "        subsetcolumn = metadata_columns[self.level-1] if self.level > 0 else None\n",
    "        subsetcriterium = self.name\n",
    "\n",
    "        # Filter out cells if subsetting necessary, keep only data from given criterium\n",
    "        if subsetcolumn != None:\n",
    "            # Reindex norm_express and labels based on cell names in given criterium\n",
    "            temp = labels.loc[labels[subsetcolumn] == subsetcriterium]\n",
    "            labels = labels.reindex(index=temp.index)\n",
    "            norm_express = norm_express.reindex(index=temp.index)\n",
    "        # print (labels[labelcolumn].value_counts())\n",
    "        # Remove cells with labels not provided in the dictionary, replace present keys with values\n",
    "        temp = labels.loc[labels[labelcolumn].isin(list(self.labeldict.values()))]\n",
    "        labels = labels.reindex(index=temp.index)\n",
    "        norm_express = norm_express.reindex(index=temp.index)\n",
    "        print (labels[labelcolumn].value_counts())\n",
    "        for i in range(len(self.labeldict)):\n",
    "            labels = labels.replace(self.labeldict[i],i)\n",
    "\n",
    "        # If validation with metadata, testsplit not provided, return all data w/o train test split\n",
    "        if testsplit is None:\n",
    "            X = norm_express.values\n",
    "            labels.index.name = 'cells'\n",
    "            labels.reset_index(inplace=True)\n",
    "            Y = labels.values\n",
    "            all_cellnames = Y[:,0]\n",
    "            all_cellnames = all_cellnames.ravel()\n",
    "            Y = labels[labelcolumn].values\n",
    "            return X, Y, all_cellnames\n",
    "\n",
    "        X = norm_express.values\n",
    "\n",
    "        # Get cell names in 10% test set - make cell names into a new column to save in a list\n",
    "        labels.index.name = 'cells'\n",
    "        labels.reset_index(inplace=True)\n",
    "        Y = labels.values\n",
    "        np.random.seed(0)\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X_tr, X_test, Y_tr, Y_test = train_test_split(X, Y, test_size=testsplit, random_state=42, shuffle = True, stratify = Y[:,self.level+1])\n",
    "        test_cellnames = Y_test[:,0]\n",
    "        test_cellnames = test_cellnames.ravel()\n",
    "\n",
    "        # Remake train and test split to save X, Y, X_tr, X_test, Y_tr, Y_test\n",
    "        X = norm_express.values\n",
    "        Y = labels[labelcolumn].values\n",
    "        np.random.seed(0)\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X_tr, X_test, Y_tr, Y_test = train_test_split(X, Y, test_size=testsplit, random_state=42, shuffle = True, stratify = Y)\n",
    "        print('Training Samples: ' + str(len(X_tr)) + ', Testing Samples: ' + str(len(X_test)))\n",
    "        return X, Y, X_tr, X_test, Y_tr, Y_test, test_cellnames;\n",
    "\n",
    "\n",
    "    # Calculates metrics of the Layer model on a provided test set and outputs it in a file\n",
    "    # cv_final_val is a naming string to differentiate between cv, final, and validation metrics\n",
    "    def model_metrics(self, cv_final_val, rejectcutoff, X_test, Y_test):\n",
    "        self.add_dictentry('Unclassified')\n",
    "        d_test = xgb.DMatrix(X_test, Y_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        for i in range(len(probabilities_xgb)):\n",
    "            if probabilities_xgb[i,probabilities_xgb.argmax(axis=1)[i]] < rejectcutoff:\n",
    "                predictions_xgb[i] = len(self.labeldict)-1\n",
    "        target_names = [str(x) for x in sorted(list(set(Y_test).union(predictions_xgb)))]\n",
    "        metrics = classification_report(Y_test, predictions_xgb, target_names=target_names)\n",
    "        with open(path + self.name + '_' + cv_final_val + 'metrics.txt', 'a+') as f:\n",
    "            print(metrics, file=f)\n",
    "        del self.labeldict[len(self.labeldict)-1]\n",
    "\n",
    "\n",
    "    # Outputs predictions of the Layer model on a provided test set\n",
    "    # Adds a key value pair in the labeldict for an unclassified class, removes it when completed\n",
    "    # 1 csv file outputted for each rejection cutoff in the list rejectioncutoffs\n",
    "    # A cutoff of 0.5 means the probability of the label must be >=0.5 for a prediction to be called\n",
    "    # A cutoff of 0 is equivalent to no rejection option\n",
    "    # train_val is a naming string to differentiate between train/test predictions and validation predictions\n",
    "    def cell_predictions(self, train_val, rejectcutoffs, test_cellnames, X_test, Y_test=None):\n",
    "        d_test = xgb.DMatrix(X_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        self.add_dictentry('Unclassified')\n",
    "        for cutoff in rejectcutoffs:\n",
    "            counter = 0\n",
    "            for i in range(len(probabilities_xgb)):\n",
    "                if probabilities_xgb[i,probabilities_xgb.argmax(axis=1)[i]] < cutoff:\n",
    "                    predictions_xgb[i] = len(self.labeldict)-1\n",
    "                    counter += 1\n",
    "            print('Unclassified # of cells w/ probability cutoff=' + str(cutoff) + ': ' + str(counter))\n",
    "            f = open(path + self.name + '_' + train_val + 'predictions_reject' + str(cutoff) + '.csv','w')\n",
    "            if Y_test is not None:\n",
    "                f.write('Cell ID,True Label,Predicted Label')\n",
    "            else:\n",
    "                f.write('Cell ID,Predicted Label')\n",
    "            for j in range(len(self.labeldict)-1):\n",
    "                f.write(',')\n",
    "                f.write(self.labeldict[j] + ' Probability')\n",
    "            f.write('\\n')\n",
    "            for i in range(test_cellnames.size):\n",
    "                f.write(test_cellnames[i])\n",
    "                f.write(',')\n",
    "                if Y_test is not None:\n",
    "                    f.write(self.labeldict[Y_test[i]])\n",
    "                    f.write(',')\n",
    "                f.write(self.labeldict[predictions_xgb[i]])\n",
    "                for j in range(len(self.labeldict)-1):\n",
    "                    f.write(',')\n",
    "                    f.write(str(probabilities_xgb[i][j]))\n",
    "                f.write('\\n')\n",
    "            f.close()\n",
    "        del self.labeldict[len(self.labeldict)-1]\n",
    "\n",
    "\n",
    "    # Outputs a confusion matrix of the Layer model's results on a provided test set\n",
    "    # train_val is a naming string to differentiate between train/test predictions and validation predictions\n",
    "    def cfsn_mtx(self, train_val, rejectcutoff, X_test, Y_test):\n",
    "        d_test = xgb.DMatrix(X_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        self.add_dictentry('Unclassified')\n",
    "        for i in range(len(probabilities_xgb)):\n",
    "            if probabilities_xgb[i,probabilities_xgb.argmax(axis=1)[i]] < rejectcutoff:\n",
    "                predictions_xgb[i] = len(self.labeldict)-1\n",
    "        cm = confusion_matrix(Y_test, predictions_xgb)\n",
    "        print(cm)\n",
    "        del self.labeldict[len(self.labeldict)-1]\n",
    "\n",
    "        for i in range(len(predictions_xgb)-1, -1, -1):\n",
    "            if predictions_xgb[i] == len(self.labeldict):\n",
    "                predictions_xgb = np.delete(predictions_xgb, i)\n",
    "                Y_test = np.delete(Y_test, i)\n",
    "        cm_removed = confusion_matrix(Y_test, predictions_xgb)\n",
    "        print(cm_removed)\n",
    "        classnames = [self.labeldict[x] for x in sorted(list(set(Y_test).union(predictions_xgb)))]\n",
    "        print(classnames)\n",
    "        cm_removed = cm_removed.astype('float') / cm_removed.sum(axis=1)[:, np.newaxis]\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.imshow(cm_removed, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classnames))\n",
    "        plt.xticks(tick_marks, classnames, rotation=45)\n",
    "        plt.yticks(tick_marks, classnames)\n",
    "\n",
    "        thresh = cm_removed.max() / 2.\n",
    "        for i, j in itertools.product(range(cm_removed.shape[0]), range(cm_removed.shape[1])):\n",
    "            plt.text(j, i, '%0.3f' % cm_removed[i,j] if cm_removed[i,j] > 0 else 0,\n",
    "                horizontalalignment='center', verticalalignment='center',\n",
    "                color='white' if cm_removed[i,j] > thresh else 'black', fontsize=6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path + self.name + '_' + train_val + 'confusionmatrix.svg')\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "    # Creates ROC curves for the Layer model on a provided test set\n",
    "    # Calculates curves using probability values and thresholds\n",
    "    # Outputs 3 figures: micro and macro ROC curves, per class ROC curves, and all combined\n",
    "    def roc_curves(self, X_test, Y_test):\n",
    "        d_test = xgb.DMatrix(X_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        n_classes = len(self.labeldict)\n",
    "        lw = 2\n",
    "\n",
    "        # One hot encode Y_test and predictions arrays\n",
    "        y_test = np.zeros((Y_test.size, Y_test.max()+1))\n",
    "        y_test[np.arange(Y_test.size),Y_test] = 1\n",
    "        y_score = probabilities_xgb\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        # Plot micro/macro ROC curves\n",
    "        plt.figure()\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]),\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]),\n",
    "                 color='navy', linestyle=':', linewidth=4)\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Overall ROC Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(path + self.name + '_miacroroc.svg')\n",
    "        plt.clf()\n",
    "\n",
    "        # Plot class ROC curves\n",
    "        plt.figure()\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], color='#%06X' % random.randint(0, 0xFFFFFF), lw=lw,\n",
    "                     label='ROC curve of {0} (area = {1:0.2f})'\n",
    "                     ''.format(self.labeldict[i], roc_auc[i]))\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Class ROC Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(path + self.name + '_classroc.svg')\n",
    "        plt.clf()\n",
    "\n",
    "        # Plot all ROC curves\n",
    "        plt.figure()\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], color='#%06X' % random.randint(0, 0xFFFFFF), lw=lw,\n",
    "                     label='ROC curve of {0} (area = {1:0.2f})'\n",
    "                     ''.format(self.labeldict[i], roc_auc[i]))\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]),\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]),\n",
    "                 color='navy', linestyle=':', linewidth=4)\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('All ROC Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(path + self.name + '_allroc.svg')\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "    # Creates precision-recall curves for the Layer model on a provided test set\n",
    "    # Calculates curves using probability values and thresholds\n",
    "    # Outputs 1 figure: per class PR curvs and a micro PR curve\n",
    "    def pr_curves(self, X_test, Y_test):\n",
    "        d_test = xgb.DMatrix(X_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        n_classes = len(self.labeldict)\n",
    "        lw = 2\n",
    "\n",
    "        # One hot encode Y_test and predictions arrays\n",
    "        y_test = np.zeros((Y_test.size, Y_test.max()+1))\n",
    "        y_test[np.arange(Y_test.size),Y_test] = 1\n",
    "        y_score = probabilities_xgb\n",
    "\n",
    "        # Compute precision and recall for each class\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        average_precision = dict()\n",
    "        for i in range(n_classes):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_score[:, i])\n",
    "            average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "\n",
    "        # Compute micro-average precision and recall\n",
    "        precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(), y_score.ravel())\n",
    "        average_precision[\"micro\"] = average_precision_score(y_test, y_score, average=\"micro\")\n",
    "\n",
    "        # Plot micro + class PR curves\n",
    "        plt.figure()\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(recall[i], precision[i], color='#%06X' % random.randint(0, 0xFFFFFF), lw=lw,\n",
    "                     label='PR curve of {0} (area = {1:0.2f})'\n",
    "                     ''.format(self.labeldict[i], average_precision[i]))\n",
    "        plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
    "                 label='micro-average PR curve (area = {0:0.2f})'\n",
    "                       ''.format(average_precision[\"micro\"]),\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('PR Curves')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.savefig(path + self.name + '_allpr.svg')\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "\n",
    "Reads in user input, selects a pathway, and trains / predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ## CELLPY RUN OPTIONS\n",
    "    # 1a. training w/ cross validation and metrics\n",
    "    #       (runMode = trainAll, trainNormExpr, labelInfo, trainMetadata, testSplit, rejectionCutoff)\n",
    "    # 1b. training w/o cross validation and metrics\n",
    "    #       (runMode = trainAll, trainNormExpr, labelInfo, trainMetadata, rejectionCutoff)\n",
    "    # 2a. prediction w/ metadata\n",
    "    #       (runMode = predictOne, predNormExpr, predMetadata, layerObjectPaths, rejectionCutoff)\n",
    "    # 2b. prediction w/o metadata, each layer's prediction independent of predictions from other layers\n",
    "    #       (runMode = predictOne, predNormExpr, layerObjectPaths, rejectionCutoff)\n",
    "    # 3a. cardiac dev prediction w/ metadata\n",
    "    #       (runMode = predictOne, predNormExpr, predMetadata, layerObjectPaths = cardiacDevAtlas, rejectionCutoff, timePoint)\n",
    "    # 3b. cardiac dev prediction w/o metadata, each layer's prediction independent of predictions from other layers\n",
    "    #       (runMode = predictOne, predNormExpr, layerObjectPaths = cardiacDevAtlas, rejectionCutoff)\n",
    "    # 4a. prediction w/o metadata, each layer's prediction influences next layer's prediction\n",
    "    #       (runMode = predictAll, predNormExpr, layerObjectPaths, rejectionCutoff)\n",
    "    # 4a. cardiac dev prediction w/o metadata, each layer's prediction influences next layer's prediction\n",
    "    #       (runMode = predictAll, predNormExpr, layerObjectPaths = cardiacDevAtlas, rejectionCutoff, timePoint)\n",
    "    # 5.  feature ranking\n",
    "    #       (runMOde = featureRankingOne, trainNormExpr, trainMetadata, layerObjectPaths, featureRankingSplit)\n",
    "    time_start = time.perf_counter()\n",
    "\n",
    "    # All variables used for training and prediction set to None\n",
    "    global path, path_cda, rejection_cutoff\n",
    "    path = os.getcwd()\n",
    "    path_cda = os.getcwd()\n",
    "    user_train = False\n",
    "    user_predictOne = False\n",
    "    user_predictAll = False\n",
    "    user_fr = False\n",
    "    train_normexpr = None\n",
    "    labelinfo = None\n",
    "    train_metadata = None\n",
    "    testsplit = None\n",
    "    rejection_cutoff = None\n",
    "    pred_normexpr = None\n",
    "    pred_metadata = None\n",
    "    layer_paths = None\n",
    "    cardiac_dev = False\n",
    "    time_point = None\n",
    "    frsplit = None\n",
    "\n",
    "    ## Command Line Interface\n",
    "    # runMode must be 'trainAll', 'predictOne', 'predictAll', or 'featureRankingOne'\n",
    "    # trainNormExpr, labelInfo, trainMetadata are paths to their respective training files\n",
    "    # testSplit is a float between 0 and 1 denoting the percentage of data to holdout for testing\n",
    "    #           if not provided, cross validation is skipped, 100% model trained w/o metrics\n",
    "    # rejectionCutoff is a float between 0 and 1 denoting the minimum probability for a prediction to not be rejected\n",
    "    # predNormExpr, predMetadata are paths to their respective prediction files\n",
    "    # layerObjectPaths is a comma-separated list of paths to the Layer objects that the user wants to predict on the predNormExpr\n",
    "    #           if the user wishes to use the pretrained cardiac developmental cell atlas objects,\n",
    "    #           the option cardiacDevAtlas should be submitted for layerObjectPaths instead\n",
    "    # timePoint is only required if the cardiacDevAtlas option is selected for layerObjectPaths\n",
    "    #           should be between 7.5 and 14, the age of the cardiac cells in the normalized expression matrix\n",
    "    # featureRankingSplit is a float between 0 and 1 denoting the percentage of data to calculate SHAP importances\n",
    "    args = sys.argv[1:]\n",
    "    options, args = getopt.getopt(args, '',\n",
    "                        ['runMode=', 'trainNormExpr=', 'labelInfo=', 'trainMetadata=', 'testSplit=', 'rejectionCutoff=',\n",
    "                         'predNormExpr=', 'predMetadata=', 'layerObjectPaths=', 'timePoint=', 'featureRankingSplit='])\n",
    "    for name, value in options:\n",
    "        if name in ['--runMode']:\n",
    "            if value == 'trainAll':\n",
    "                user_train = True\n",
    "            elif value == 'predictOne':\n",
    "                user_predictOne = True\n",
    "            elif value == 'predictAll':\n",
    "                user_predictAll = True\n",
    "            elif value == 'featureRankingOne':\n",
    "                user_fr = True\n",
    "        if name in ['--trainNormExpr']:\n",
    "            train_normexpr = value\n",
    "        if name in ['--labelInfo']:\n",
    "            labelinfo = value\n",
    "        if name in ['--trainMetadata']:\n",
    "            train_metadata = value\n",
    "        if name in ['--testSplit']:\n",
    "            testsplit = float(value)\n",
    "        if name in ['--rejectionCutoff']:\n",
    "            rejection_cutoff = float(value)\n",
    "        if name in ['--predNormExpr']:\n",
    "            pred_normexpr = value\n",
    "        if name in ['--predMetadata']:\n",
    "            pred_metadata = value\n",
    "        if name in ['--layerObjectPaths']:\n",
    "            if value == 'cardiacDevAtlas':\n",
    "                cardiac_dev = True\n",
    "            else:\n",
    "                layer_paths = value.split(',')\n",
    "        if name in ['--timePoint']:\n",
    "            time_point = float(value)\n",
    "        if name in ['--featureRankingSplit']:\n",
    "            frsplit = float(value)\n",
    "\n",
    "    # Check user provided variables follow an above cellpy pathway\n",
    "    passed_options = check_combinations(user_train, user_predictOne, user_predictAll, user_fr, train_normexpr, labelinfo, train_metadata, testsplit,\n",
    "                                        rejection_cutoff, pred_normexpr, pred_metadata, layer_paths, cardiac_dev, time_point, frsplit)\n",
    "    if passed_options is False:\n",
    "        raise ValueError('see printed error log above')\n",
    "\n",
    "    # Create cellpy_results directory with timestamp\n",
    "    newdir = 'cellpy_results_' + datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    path = os.path.join(path, newdir)\n",
    "    if not os.path.isdir(path):\n",
    "        print('Created directory \"cellpy_results\" in cwdir: ' + path)\n",
    "        os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "\n",
    "    # Check training files exist if training option called\n",
    "    passed_train = None\n",
    "    passed_predict = None\n",
    "    passed_fr = None\n",
    "    if user_train is True:\n",
    "        passed_train = check_trainingfiles(train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff)\n",
    "    # Check prediction files exist if either of the prediction options is called\n",
    "    if (user_predictOne is True) or (user_predictAll is True):\n",
    "        passed_predict = check_predictionfiles(pred_normexpr, pred_metadata, layer_paths, time_point)\n",
    "    # Check feature ranking files exist if feature ranking option called\n",
    "    if user_fr is True:\n",
    "        passed_fr = check_featurerankingfiles(train_normexpr, train_metadata, layer_paths, frsplit)\n",
    "    if (passed_train is False) or (passed_predict is False) or (passed_fr is False):\n",
    "        raise ValueError('see printed error log above')\n",
    "\n",
    "    # Initialize layer_paths if cardiacDevAtlas option is selected\n",
    "    if cardiac_dev is True:\n",
    "        layer_paths = [path_cda + '/CellPy-main/cardiacdevatlas_objects/Root_object.pkl',\n",
    "                       path_cda + '/CellPy-main/cardiacdevatlas_objects/Cardiomyocytes_object.pkl']\n",
    "        if time_point < 8:\n",
    "            print('Ventricular cardiomyocyte model E7.75 will be used for prediction')\n",
    "            layer_paths.append(path_cda + '/CellPy-main/cardiacdevatlas_objects/E7.75_object.pkl')\n",
    "        elif time_point < 9:\n",
    "            print('Ventricular cardiomyocyte model E8.25 will be used for prediction')\n",
    "            layer_paths.append(path_cda + '/CellPy-main/cardiacdevatlas_objects/E8.25_object.pkl')\n",
    "        elif time_point < 10:\n",
    "            print('Ventricular cardiomyocyte model E9.25 will be used for prediction')\n",
    "            layer_paths.append(path_cda + '/CellPy-main/cardiacdevatlas_objects/E9.25_object.pkl')\n",
    "        elif time_point < 12:\n",
    "            print('Ventricular cardiomyocyte model E10.5 will be used for prediction')\n",
    "            layer_paths.append(path_cda + '/CellPy-main/cardiacdevatlas_objects/E10.5_object.pkl')\n",
    "        else:\n",
    "            print('Ventricular cardiomyocyte model E13.5 will be used for prediction')\n",
    "            layer_paths.append(path_cda + '/CellPy-main/cardiacdevatlas_objects/E13.5_object.pkl')\n",
    "\n",
    "    # If training option is called and feasible\n",
    "    if user_train is True and passed_train is True:\n",
    "        training(train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff)\n",
    "    # If prediction one option is called on non-cardiacDevAtlas objects and feasible\n",
    "    if user_predictOne is True and passed_predict is True and cardiac_dev is False:\n",
    "        predictionOne(pred_normexpr, pred_metadata, layer_paths)\n",
    "    # If prediction all option is called on non-cardiacDevAtlas objects and feasible\n",
    "    if user_predictAll is True and passed_predict is True and cardiac_dev is False:\n",
    "        predictionAll(pred_normexpr, layer_paths)\n",
    "    # If prediction one option is called on cardiacDevAtlas objects and feasible\n",
    "    if user_predictOne is True and passed_predict is True and cardiac_dev is True:\n",
    "        predictionOneCDA(pred_normexpr, pred_metadata, layer_paths)\n",
    "    # If prediction all option is called on cardiacDevAtlas objects and feasible\n",
    "    if user_predictAll is True and passed_predict is True and cardiac_dev is True:\n",
    "        predictionAllCDA(pred_normexpr, layer_paths)\n",
    "    # If feature ranking option is called and feasible\n",
    "    if user_fr is True and passed_fr is True:\n",
    "        featureranking(train_normexpr, train_metadata, layer_paths, frsplit)\n",
    "\n",
    "    # Print computational time and memory required\n",
    "    time_elapsed = (time.perf_counter() - time_start)\n",
    "    memMb=resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0/1024.0\n",
    "    print (\"%5.1f secs %5.1f MByte\" % (time_elapsed,memMb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Table of Contents\n",
    "\n",
    "[Table of Contents](tableofcontents.ipynb#toc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
