{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellPy Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CellPy is a Python-based method of classifying single cell RNA sequencing datasets. The classifier predicts cell types once it is trained on a reference dataset. \n",
    "\n",
    "Paper: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CellPy Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CellPy has been formatted into a wrapper function that can be easily run through the command line of the Terminal. \n",
    "\n",
    "### CellPy Run Option Summary and Examples\n",
    "\n",
    "1a. TrainOnly: training w/ cross validation and metrics\n",
    "* (runMode = trainOnly, trainNormExpr, labelInfo, trainMetadata, testSplit, rejectionCutoff)\n",
    "* Example: `cellpy --runMode trainOnly --trainNormExpr /cellpy_example/zheng_pbmc_10K.csv --labelInfo /cellpy_example/zheng_labelinfo.csv --trainMetadata /cellpy_example/zheng_pbmc_10K_metadata.csv --testSplit 0.1 --rejectionCutoff 0.5`\n",
    "\n",
    "1b. TrainOnly: training w/o cross validation and metrics\n",
    "* (runMode = trainOnly, trainNormExpr, labelInfo, trainMetadata, rejectionCutoff)\n",
    "\n",
    "2a. PredictionOnly: prediction w/ val_metadata\n",
    "* (runMode = predictionOnly, predNormExpr, predMetadata, layerObjectPaths, rejectionCutoff)\n",
    "\n",
    "2b. PredictionOnly: prediction w/o val_metadata\n",
    "* (runMode = predictionOnly, predNormExpr, layerObjectPaths, rejectionCutoff)\n",
    "\n",
    "3a. FeatureRankingOnly\n",
    "* (runMOde = featureRankingOnly, trainNormExpr, trainMetadata, layerObjectPaths, featureRankingSplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Only Option\n",
    "\n",
    "Required User Inputs:\n",
    "-  **train_normexpr**: normalized expression matrix csv file\n",
    "-  **labelinfo**: label information csv file\n",
    "-  **train_metadata**: metadata csv file\n",
    "-  **testsplit (optional)**: \n",
    "-  **rejection_cutoff**: float between 0 and 1 denoting the minimum probability for a prediction to not be rejected\n",
    "\n",
    "#### Train_Normexpr\n",
    "\n",
    "FORMAT: csv file\n",
    "\n",
    "#### Labelinfo\n",
    "\n",
    "FORMAT: csv file\n",
    "\n",
    "#### Train_Metadata\n",
    "\n",
    "FORMAT: csv file\n",
    "\n",
    "#### Testsplit (Optional)\n",
    "\n",
    "FORMAT: float between 0 and 1\n",
    "\n",
    "#### Rejection_Cutoff\n",
    "\n",
    "FORMAT: float between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Only Option\n",
    "\n",
    "Required User Inputs:\n",
    "-  **val_normexpr**: normalized expression matrix file\n",
    "-  **val_metadata (optional)**: metadata file\n",
    "-  **layer_paths**: a list of path names to Layer objects\n",
    "-  **rejection_cutoff**: float between 0 and 1 denoting the minimum probability for a prediction to not be rejected\n",
    "\n",
    "#### Val_Normexpr\n",
    "#### Val_Metadata (Optional)\n",
    "#### Layer_Paths\n",
    "#### Rejection_Cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Ranking Only Option\n",
    "\n",
    "Required User Inputs:\n",
    "-  **train_normexpr**: normalized expression matrix file\n",
    "-  **train_metadata**: metadata file\n",
    "-  **layer_paths**: a list of path names to Layer objects\n",
    "-  **frsplit(optional)**: float between 0 and 1 denoting the percentage of data to calculate SHAP importances, default is 0.3\n",
    "\n",
    "#### Train_Normexpr\n",
    "#### Train_Metadata\n",
    "#### Layer_Paths\n",
    "#### Frsplit (Optional, Default = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CellPy Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "import resource \n",
    "import sys\n",
    "import getopt\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from numpy import interp\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensures given files satisfy one of the possible pathways provided by cellpy\n",
    "Ensures user input for train or predict matches file inputs\n",
    "Certain files must appear together for training and/or validation to proceed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_combinations(user_train, user_predict, user_fr, train_normexpr, labelinfo, train_metadata, testsplit,\n",
    "                       rejection_cutoff, val_normexpr, val_metadata, layer_paths, frsplit):\n",
    "    passed = True\n",
    "    if user_train is None:\n",
    "        print('ERROR: Run mode must be provided to resume')\n",
    "\n",
    "    # if the user selected the 'trainOnly' option\n",
    "    train_list = [train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff]\n",
    "    if user_train is True:\n",
    "        print('Training option selected')\n",
    "        if train_list[0] is None:\n",
    "            print('ERROR: Normalized expression matrix for training must be provided to resume')\n",
    "            passed = False\n",
    "        if train_list[1] is None:\n",
    "            print('ERROR: Label information file must be provided to resume')\n",
    "            passed = False\n",
    "        if train_list[2] is None:\n",
    "            print('ERROR: Metadata file for training must be provided to resume')\n",
    "            passed = False\n",
    "        if train_list[3] is None:\n",
    "            print('WARNING: Test split amount not provided, training will proceed w/o cross-validation and metric calculations')\n",
    "        if train_list[4] is None:\n",
    "            print('ERROR: Rejection cutoff value must be provided to resume')\n",
    "            passed = False\n",
    "\n",
    "    # if the user selected the 'predictOnly' option\n",
    "    predict_list = [val_normexpr, val_metadata, layer_paths, rejection_cutoff]\n",
    "    if user_predict is True:\n",
    "        if val_metadata is not None:\n",
    "            print('Prediction option with accuracy calculation selected')\n",
    "        else:\n",
    "            print('Prediction option without accuracy calculation selected')\n",
    "        if predict_list[0] is None:\n",
    "            print('ERROR: Normalized expression matrix for prediction must be provided to resume')\n",
    "            passed = False\n",
    "        if predict_list[2] is None:\n",
    "            print('ERROR: Path names to Layer objects must be provided to resume')\n",
    "            passed = False\n",
    "        if predict_list[3] is None:\n",
    "            print('ERROR: Rejection cutoff value must be provided to resume')\n",
    "            passed = False\n",
    "\n",
    "    # if the user selected the 'featureRankingOnly' option\n",
    "    fr_list = [train_normexpr, train_metadata, layer_paths, frsplit]\n",
    "    if user_fr is True:\n",
    "        if fr_list[0] is None:\n",
    "            print('ERROR: Normalized expression matrix for training must be provided to resume')\n",
    "            passed = False\n",
    "        if fr_list[1] is None:\n",
    "            print('ERROR: Metadata file for training must be provided to resume')\n",
    "            passed = False\n",
    "        if fr_list[2] is None:\n",
    "            print('ERROR: Path names to Layer objects must be provided to resume')\n",
    "            passed = False\n",
    "        if fr_list[3] is None:\n",
    "            print('WARNING: Feature ranking split not provided but feature ranking on, will be automatically set to 0.3')\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensures all the user given variables for training exist or are in bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trainingfiles(train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff):\n",
    "    passed = True\n",
    "    if not os.path.exists(train_normexpr):\n",
    "        print('ERROR: Given normalized expression data file for training does not exist')\n",
    "        passed = False\n",
    "    if not os.path.exists(labelinfo):\n",
    "        print('ERROR: Given label info file does not exist')\n",
    "        passed = False\n",
    "    if not os.path.exists(train_metadata):\n",
    "        print('ERROR: Given metadata file for training does not exist')\n",
    "        passed = False\n",
    "    if testsplit is not None and (testsplit > 1 or testsplit < 0):\n",
    "        print('ERROR: Given test split percentage must be a value between 0 and 1')\n",
    "        passed = False\n",
    "    if rejection_cutoff > 1 or rejection_cutoff < 0:\n",
    "        print('ERROR: Given rejection cutoff must be a value between 0 and 1')\n",
    "        passed = False\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducts training in all layers separated into different folders by name\n",
    "Creates directory 'training' in cellpy_results folder, defines 'Root' as topmost layer\n",
    "Conducts finetuning on Root layer with 50 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff):\n",
    "    global path\n",
    "    path = os.path.join(path, 'training')\n",
    "    os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    csv2pkl(train_normexpr)\n",
    "    train_normexpr = train_normexpr[:-3] + 'pkl'\n",
    "    all_layers = [Layer('Root', 0)]\n",
    "    construct_tree(labelinfo, all_layers)\n",
    "    print(all_layers)\n",
    "    for layer in all_layers:\n",
    "        path = os.path.join(path, layer.name.replace(' ', ''))\n",
    "        os.mkdir(path)\n",
    "        os.chdir(path)\n",
    "        path = path + '/'\n",
    "        if layer.name == 'Root': # root top layer\n",
    "            parameters = layer.finetune(2, testsplit, train_normexpr, train_metadata)\n",
    "            print(parameters)\n",
    "        layer.train_layer(train_normexpr, train_metadata, parameters, testsplit, [0, rejection_cutoff])\n",
    "        os.chdir('..') # return to training directory\n",
    "        path = os.getcwd()\n",
    "    path = path + '/'\n",
    "    training_summary(all_layers)\n",
    "    export_layers(all_layers)\n",
    "    print('Training Complete')\n",
    "    os.chdir('..') # return to cellpy directory\n",
    "    path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts the normalized expression csv into a pkl\n",
    "Expression CSV file must contain genes as row names, samples as column names\n",
    "First column name (cell A1) is 'gene'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2pkl(csvpath):\n",
    "    tp = pd.read_csv(csvpath, iterator=True, chunksize=1000)\n",
    "    norm_express = pd.concat(tp, ignore_index=True)\n",
    "    print (norm_express.head())\n",
    "    norm_express.set_index('gene', inplace=True)\n",
    "    norm_express.index.names = [None]\n",
    "    norm_express = norm_express.T\n",
    "    norm_express.to_pickle(csvpath[:-3] + 'pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructs a list of all Layer objects from a labelinfo file\n",
    "Initalizes each Layer object with a name, level #, and label dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree(labelinfo, all_layers):\n",
    "    labeldata = fill_data(labelinfo)\n",
    "    fill_dict(labeldata, all_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function of construct_tree, fills the vertical columns of the labeldata file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(labelinfo):\n",
    "    labeldata = list(csv.reader(open(labelinfo, encoding='utf-8-sig')))\n",
    "    for i in range(1,len(labeldata)):\n",
    "        if labeldata[i][0]=='':\n",
    "            labeldata[i][0] = labeldata[i-1][0]\n",
    "    for i in range(1,len(labeldata)):\n",
    "        for j in range(1,len(labeldata[0])):\n",
    "            if labeldata[i][j]=='' and labeldata[i][j-1]==labeldata[i-1][j-1]:\n",
    "                labeldata[i][j] = labeldata[i-1][j]\n",
    "    return labeldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function of construct_tree after fill_data, reads the edited / filled labeldata file\n",
    "Constructs the main list structure and initializes all Layer objects in it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dict(labeldata, all_layers):\n",
    "    for i in range(len(labeldata)):\n",
    "        # Fills the root dictionary\n",
    "        if find_layer(all_layers, labeldata[i][0]) is None:\n",
    "            root_layer = find_layer(all_layers, 'Root')\n",
    "            root_layer.add_dictentry(labeldata[i][0])\n",
    "        # Fills dictionaries in other layers given the existence of a leaf\n",
    "        for j in range(1,len(labeldata[0])):\n",
    "            if labeldata[i][j]!='':\n",
    "                if find_layer(all_layers, labeldata[i][j-1]) is None:\n",
    "                    all_layers.append(Layer(labeldata[i][j-1], j))\n",
    "                prev_layer = find_layer(all_layers, labeldata[i][j-1])\n",
    "                prev_layer.add_dictentry(labeldata[i][j])\n",
    "            else:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function of fill_dict, searches all_layers for a layer with the given name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_layer(all_layers, name):\n",
    "    for layer in all_layers:\n",
    "        if layer.name == name:\n",
    "            return layer\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizes and lists the path names of all the files created during training\n",
    "Prints the summary of each Layer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_summary(all_layers):\n",
    "    f = open(path + 'training_summaryfile.txt', 'w')\n",
    "    for layer in all_layers:\n",
    "        f.write(str(layer))\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exports all the trained Layers as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_layers(all_layers):\n",
    "    for layer in all_layers:\n",
    "        with open(path + layer.name + '_object.pkl', 'wb') as output:\n",
    "            pickle.dump(layer, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensures all the user given variables for validation exist / are in the correct format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_predictionfiles(val_normexpr, val_metadata=None, layer_paths=None):\n",
    "    passed = True\n",
    "    if not os.path.exists(val_normexpr):\n",
    "        print('ERROR: Given validation normalized expression data file for prediction does not exist')\n",
    "        passed = False\n",
    "    if val_metadata != None and not os.path.exists(val_metadata):\n",
    "        print('ERROR: Given validation metadata file for prediction does not exist')\n",
    "        passed = False\n",
    "    # check all layer paths are objects and contain a trained xgb model\n",
    "    if layer_paths != None:\n",
    "        for i in range(len(layer_paths)):\n",
    "            layer_path = layer_paths[i]\n",
    "            if not os.path.exists(layer_path):\n",
    "                print('ERROR: Given Layer object ' + str(i) + ' does not exist')\n",
    "                passed = False\n",
    "            else:\n",
    "                layer = pd.read_pickle(layer_path)\n",
    "                if layer.trained() is False:\n",
    "                    print('ERROR: Given Layer object ' + str(i) + ' is not trained')\n",
    "                    passed = False\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducts prediction in all layers separated into different folders by name\n",
    "Creates directory 'prediction' in cellpy_results folder, defines 'Root' as topmost layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(val_normexpr, val_metadata, object_paths):\n",
    "    global path\n",
    "    path = os.path.join(path, 'prediction')\n",
    "    os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    all_layers = import_layers(object_paths)\n",
    "    featurenames = all_layers[0].xgbmodel.feature_names\n",
    "    reorder_pickle(val_normexpr, featurenames)\n",
    "    val_normexpr = val_normexpr[:-3] + 'pkl'\n",
    "    for layer in all_layers:\n",
    "        path = os.path.join(path, layer.name.replace(' ', ''))\n",
    "        os.mkdir(path)\n",
    "        os.chdir(path)\n",
    "        path = path + '/'\n",
    "        layer.predict_layer([0, rejection_cutoff], val_normexpr, val_metadata)\n",
    "        os.chdir('..') # return to prediction directory\n",
    "        path = os.getcwd()\n",
    "    print('Validation Complete')\n",
    "    os.chdir('..') # return to cellpy directory\n",
    "    path = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports Layer objects from a list of given paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_layers(layer_paths):\n",
    "    layers = []\n",
    "    for layer_path in layer_paths:\n",
    "         layer = pd.read_pickle(layer_path)\n",
    "         layers.append(layer)\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts the normalized expression csv into a pkl\n",
    "Expression CSV file must contain genes as row names, samples as column names\n",
    "First column name (cell A1) is 'gene'\n",
    "Reorders the csv file to match the features in a given featurenames list\n",
    "Returns path to the new pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_pickle(csvpath, featurenames):\n",
    "    tp = pd.read_csv(csvpath, iterator=True, chunksize=1000)\n",
    "    norm_express = pd.concat(tp, ignore_index=True)\n",
    "    print (norm_express.head())\n",
    "    norm_express.set_index('gene', inplace=True)\n",
    "    norm_express.index.names = [None]\n",
    "    norm_express = norm_express.T\n",
    "    print(norm_express.T.duplicated().any())\n",
    "    print ('Training Data # of  genes: ' + str(len(featurenames)))\n",
    "\n",
    "    ## Manually reorder columns according to training data index\n",
    "    # Reorder overlapping genes, remove genes not in training data\n",
    "    origfeat = list(norm_express)\n",
    "    print ('Validation Data # of genes: ' + str(len(origfeat)))\n",
    "    newindex = []\n",
    "    for i in range(len(featurenames)):\n",
    "        if featurenames[i] in origfeat:\n",
    "            newindex.append(featurenames[i])\n",
    "    print ('Overlapping # of genes: ' + str(len(newindex)))\n",
    "    norm_express = norm_express.reindex(columns=newindex)\n",
    "    # Add missing features, remove extra features to match atlas\n",
    "    i = 0\n",
    "    missing_counter = 0\n",
    "    while i < len(list(norm_express)):\n",
    "        if list(norm_express)[i] != featurenames[i]:\n",
    "            norm_express.insert(i, featurenames[i], None)\n",
    "            missing_counter += 1\n",
    "        i += 1\n",
    "    while i < len(featurenames):\n",
    "        norm_express.insert(i, featurenames[i], None)\n",
    "        i += 1\n",
    "        missing_counter += 1\n",
    "    # Overlapping + missing = training total\n",
    "    print ('Missing # of genes: ' + str(missing_counter))\n",
    "    norm_express.to_pickle(csvpath[:-3] + 'pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check_featureRankingfiles def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_featurerankingfiles(train_normexpr, train_metadata, layer_paths, frsplit):\n",
    "    passed = True\n",
    "    if not os.path.exists(train_normexpr):\n",
    "        print('ERROR: Given normalized expression data file for training does not exist')\n",
    "        passed = False\n",
    "    if not os.path.exists(train_metadata):\n",
    "        print('ERROR: Given metadata file for training does not exist')\n",
    "        passed = False\n",
    "    # check all layer paths are objects and contain a trained xgb model\n",
    "    if layer_paths != None:\n",
    "        for i in range(len(layer_paths)):\n",
    "            layer_path = layer_paths[i]\n",
    "            if not os.path.exists(layer_path):\n",
    "                print('ERROR: Given Layer object ' + str(i) + ' does not exist')\n",
    "                passed = False\n",
    "            else:\n",
    "                layer = pd.read_pickle(layer_path)\n",
    "                if layer.trained() is False:\n",
    "                    print('ERROR: Given Layer object ' + str(i) + ' is not trained')\n",
    "                    passed = False\n",
    "    if frsplit is not None and (frsplit > 1 or frsplit < 0):\n",
    "        print('ERROR: Given feature ranking split must be a value between 0 and 1')\n",
    "        passed = False\n",
    "    return passed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conducts prediction in all layers separated into different folders by name\n",
    "Creates directory 'prediction' in cellpy_results folder, defines 'Root' as topmost layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureranking(train_normexpr, train_metadata, object_paths, frsplit):\n",
    "    global path\n",
    "    path = os.path.join(path, 'featureranking')\n",
    "    os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    csv2pkl(train_normexpr)\n",
    "    train_normexpr = train_normexpr[:-3] + 'pkl'\n",
    "    all_layers = import_layers(object_paths)\n",
    "    for layer in all_layers:\n",
    "        path = os.path.join(path, layer.name.replace(' ', ''))\n",
    "        os.mkdir(path)\n",
    "        os.chdir(path)\n",
    "        path = path + '/'\n",
    "        layer.featurerank_layer(train_normexpr, train_metadata, frsplit)\n",
    "        os.chdir('..') # return to prediction directory\n",
    "        path = os.getcwd()\n",
    "    print('Feature Ranking Complete')\n",
    "    os.chdir('..') # return to cellpy directory\n",
    "    path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object for each layer of the model\n",
    "Contains methods for subsetting data according to the metadata and label info files,\n",
    "                     - dividing subsetted data 90-10 and 10 fold cv,\n",
    "                     - finetuning and training each layer, calculating metrics and SHAP feature ranking,\n",
    "                     - outputting final model for validation\n",
    "Keeps track of all outputted files, stores paths and names in instance variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    # level is the column of the metadata file in which the name of the layer appears\n",
    "    # xgbmodel is a trained XGBoost classifier object\n",
    "    # cvmetrics, finalmetrics, cfm, pr are path names to those files\n",
    "    # predictions, roc, fr are lists of path names\n",
    "    def __init__(self, name, level, labeldict=None, xgbmodel=None, finetuning=None, cvmetrics=None,\n",
    "                 finalmetrics=None, predictions=None, cfm=None, roc=None, pr=None, pickle=None):\n",
    "        self.name = name\n",
    "        self.level = level\n",
    "        self.labeldict = {}\n",
    "        self.xgbmodel = None\n",
    "        self.finetuning = None\n",
    "        self.cvmetrics = None\n",
    "        self.finalmetrics = None\n",
    "        self.predictions = []\n",
    "        self.cfm = None\n",
    "        self.roc = []\n",
    "        self.pr = None\n",
    "        self.pickle = self.name + '_object.pkl'\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __eq__(self, layer2):\n",
    "        return self.name==layer2.name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<Layer: '%s', Level: %s, labeldict: %s, Trained: %s>\" % (self.name, self.level, self.labeldict, self.trained())\n",
    "\n",
    "    def __str__(self):\n",
    "        return_str = 'Object: ' + self.pickle + '\\n'\n",
    "        return_str += 'Layer: ' + self.name + '\\n'\n",
    "        return_str += 'Level: ' + str(self.level) + '\\n'\n",
    "        return_str += 'Label Dictionary: ' + str(self.labeldict) + '\\n'\n",
    "        if self.finetuned():\n",
    "            return_str += 'Finetuning Log: ' + self.finetuning + '\\n'\n",
    "        if self.trained():\n",
    "            return_str += 'XGB Model: ' + self.name + '_xgbmodel.sav' + '\\n'\n",
    "            if self.cvmetrics is not None:\n",
    "                return_str += '10-Fold Cross Validation Metrics: ' + self.cvmetrics + '\\n'\n",
    "                return_str += 'Final Metrics (10%): ' + self.finalmetrics + '\\n'\n",
    "                return_str += 'Predictions (no rejection): ' + self.predictions[0] + '\\n'\n",
    "                return_str += 'Predictions (' + str(rejection_cutoff) + ' rejection cutoff): ' + self.predictions[1] + '\\n'\n",
    "                return_str += 'Confusion Matrix (10%, no rejection): ' + self.cfm + '\\n'\n",
    "                return_str += 'Micro/Macro ROC Curves: ' + self.roc[0] + '\\n'\n",
    "                return_str += 'Per-Class ROC Curves: ' + self.roc[1] + '\\n'\n",
    "                return_str += 'All Combined ROC Curves: ' + self.roc[2] + '\\n'\n",
    "                return_str += 'Precision-Recall Graph (10%, no rejection): ' + self.pr + '\\n'\n",
    "        return return_str\n",
    "\n",
    "    def finetuned(self):\n",
    "        return self.finetuning is not None\n",
    "\n",
    "    def trained(self):\n",
    "        return self.xgbmodel is not None\n",
    "\n",
    "    # Adds a value to the label dictionary if it is not already present\n",
    "    # Utility function of fill_dict\n",
    "    def add_dictentry(self, value):\n",
    "        curr_dict = self.labeldict\n",
    "        if value not in list(curr_dict.values()):\n",
    "            curr_dict[len(curr_dict)] = value\n",
    "            self.labeldict = curr_dict\n",
    "\n",
    "\n",
    "    # Hyperparameter tuning for XGBoost using accuracy\n",
    "    # Equivalent to sklearn.model_selection.RandomizedSearchCV -- w/o cv and w/ triangular dist\n",
    "    # Parameters: eta, max_depth, subsamaple, colsample_bytree\n",
    "    # n randomized trials: triangular distribution around the default or recommended xgboost value\n",
    "    #                      rounded to a reasonable decimal place\n",
    "    # Splits data according to user-provided testplit value, NO cross validation conducted\n",
    "    # Returns parameters with lowest mean absolute error (mae) on 10% for actual training\n",
    "    # Outputs csv file with accuracy of each class and mae for all trials, does not output any other metrics\n",
    "    def finetune(self, trials, testsplit, normexprpkl, metadatacsv):\n",
    "        # If user skips cross validation, testsplit automatically set to 10% for finetuning\n",
    "        if testsplit is None:\n",
    "            testsplit = 0.1\n",
    "        X, Y, X_tr, X_test, Y_tr, Y_test, _ = self.read_data(normexprpkl, metadatacsv, testsplit)\n",
    "        min_mae = 100000000000000\n",
    "        f = open(path + self.name + '_finetuning.csv', 'a+')\n",
    "        f.write('ETA,Max Depth,Subsample,Colsample by Tree,')\n",
    "        for i in range(len(self.labeldict)-1):\n",
    "            f.write(self.labeldict[i] + ' Accuracy')\n",
    "            f.write(',')\n",
    "        f.write('MAE\\n')\n",
    "        for i in range(trials):\n",
    "            eta_temp = round(random.triangular(0,1,0.3),1)\n",
    "            max_depth_temp = round(random.triangular(4,8,6))\n",
    "            subsample_temp = round(random.triangular(0.01,1,0.5),1)\n",
    "            colsample_bytree_temp = round(random.triangular(0.01,1,0.5),1)\n",
    "            params = {'objective': 'multi:softprob', 'eta': eta_temp, 'max_depth': max_depth_temp, 'subsample': subsample_temp,\n",
    "                    'colsample_bytree': colsample_bytree_temp, 'eval_metric': 'merror', 'seed': 840}\n",
    "\n",
    "            mae, output_string = self.xgboost_model_shortver(X_tr, X_test, Y_tr, Y_test, params)\n",
    "            f.write(str(eta_temp) + ',' + str(max_depth_temp) + ',' + str(subsample_temp) + ',' + str(colsample_bytree_temp) + ',')\n",
    "            f.write(output_string + '\\n')\n",
    "            if mae < min_mae:\n",
    "                min_mae = mae\n",
    "                final_params = params\n",
    "        print(final_params)\n",
    "        self.finetuning = self.name + '_finetuning.csv'\n",
    "        return final_params\n",
    "\n",
    "    # XGBoost w/o cross validation, no output files, just accuracy score on user-provided testsplit value\n",
    "    # Returns mean absolute error (mae) as float and csv output string\n",
    "    #       Accuracy for each class and mae in a comma-separated string for classification\n",
    "    # Only for finetuning!\n",
    "    def xgboost_model_shortver(self, X_tr, X_test, Y_tr, Y_test, params):\n",
    "        params['num_class'] = len(self.labeldict)\n",
    "\n",
    "        d_tr = xgb.DMatrix(X_tr, Y_tr, feature_names=feature_names)\n",
    "        model = xgb.train(params, d_tr, 20, verbose_eval=10)\n",
    "        d_test = xgb.DMatrix(X_test, Y_test, feature_names=feature_names)\n",
    "        probabilities_xgb = model.predict(d_test)\n",
    "\n",
    "        returned_str = ''\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        cm = confusion_matrix(Y_test, predictions_xgb)\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        mae = 1-sum(cm.diagonal())/len(cm.diagonal())\n",
    "        returned_str = ','.join(map(str, cm.diagonal())) + ',' + str(mae)\n",
    "        return mae, returned_str\n",
    "\n",
    "\n",
    "    # Trains one layer in the classification\n",
    "    # Trains XGBoost for a layer of classification given parameters\n",
    "    # Splits data according to user-provided testsplit\n",
    "    # Conducts 10-fold cross validation on (1-testsplit)% of data, outputs cv metrics\n",
    "    # Retrains model on (1-testsplit)% to output metrics when tested on holdout (testsplit)%\n",
    "    # Retrains final saved model on 100% of data, returns for feature ranking\n",
    "    # Conducts feature ranking with SHAP if instructed by user on full final model\n",
    "    def train_layer(self, normexprpkl, metadatacsv, params, testsplit, rejectcutoffs):\n",
    "        params['num_class'] = len(self.labeldict)\n",
    "\n",
    "        if testsplit is not None:\n",
    "            # 10-fold CV on (1-testsplit)% of data\n",
    "            X, Y, X_tr, X_test, Y_tr, Y_test, test_cellnames = self.read_data(normexprpkl, metadatacsv, testsplit)\n",
    "            kfold = 10\n",
    "            sss = StratifiedShuffleSplit(n_splits=kfold, test_size=0.1, random_state=720)\n",
    "            for i, (train_index, test_index) in enumerate(sss.split(X_tr, Y_tr)):\n",
    "                print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "                X_train, X_valid = X_tr[train_index], X_tr[test_index]\n",
    "                Y_train, Y_valid = Y_tr[train_index], Y_tr[test_index]\n",
    "                d_train = xgb.DMatrix(X_train, Y_train, feature_names=feature_names)\n",
    "                cv_model = xgb.train(params, d_train, 20, verbose_eval=500)\n",
    "                self.xgbmodel = cv_model # temporarily set xgbmodel to cv 0.9*(1-testsplit)% model\n",
    "                self.model_metrics('cv', X_valid, Y_valid)\n",
    "                self.cvmetrics = self.name + '_cvmetrics.txt'\n",
    "\n",
    "            # 90% model, 10% testing\n",
    "            d_tr = xgb.DMatrix(X_tr, Y_tr, feature_names=feature_names)\n",
    "            temp_model = xgb.train(params, d_tr, 20, verbose_eval=500)\n",
    "            self.xgbmodel = temp_model # temporarily set xgbmodel to (1-testsplit)% model\n",
    "            self.model_metrics('final', X_test, Y_test)\n",
    "            self.finalmetrics = self.name + '_finalmetrics.txt'\n",
    "            self.cell_predictions('training', rejectcutoffs, test_cellnames, X_test, Y_test)\n",
    "            for cutoff in rejectcutoffs:\n",
    "                self.predictions.append(self.name + '_trainingpredictions_reject' + str(cutoff) + '.csv')\n",
    "            self.cfsn_mtx('training', X_test, Y_test)\n",
    "            self.cfm = self.name + '_confusionmatrix.svg'\n",
    "            self.roc_curves(X_test, Y_test)\n",
    "            self.roc.append(self.name + '_miacroroc.svg')\n",
    "            self.roc.append(self.name + '_classroc.svg')\n",
    "            self.roc.append(self.name + '_allroc.svg')\n",
    "            self.pr_curves(X_test, Y_test)\n",
    "            self.pr = self.name + '_allpr.svg'\n",
    "\n",
    "        # skip cross validation and metric calculations\n",
    "        if testsplit is None:\n",
    "            X, Y, all_cellnames = self.read_data(normexprpkl, metadatacsv)\n",
    "\n",
    "        # final 100% model\n",
    "        d_all = xgb.DMatrix(X, Y, feature_names=feature_names)\n",
    "        final_model = xgb.train(params, d_all, 20, verbose_eval=500)\n",
    "        pickle.dump(final_model, open(path + self.name + '_xgbmodel.sav', 'wb'))\n",
    "        self.xgbmodel = final_model\n",
    "\n",
    "\n",
    "    # Outputs predictions of the 90% model on the 10% test set in a given dataset\n",
    "    # 1 csv file outputted for each rejection cutoff in the list rejectioncutoffs\n",
    "    # A cutoff of 0.5 means the probability of the label must be >=0.5 for a prediction to be called\n",
    "    # A cutoff of 0 is equivalent to no rejection option\n",
    "    def predict_layer(self, rejectcutoffs, normexprpkl, metadatacsv=None):\n",
    "        X, Y, all_cellnames = self.read_data(normexprpkl, metadatacsv) # Y will be None if metadatacsv=None\n",
    "        self.cell_predictions('validation', rejectcutoffs, all_cellnames, X, Y) # can handle Y=None\n",
    "        if metadatacsv != None:\n",
    "            self.cfsn_mtx('validation', X, Y)\n",
    "            self.model_metrics('validation', X, Y)\n",
    "\n",
    "\n",
    "    # Outputs SHAP feature ranking plots overall and for each class\n",
    "    # Outputs csv table containing overall SHAP values\n",
    "    # Uses same procedure for subsetting norm_expr as read_data(...) if column & criterium are provided\n",
    "    # frsplit automatically set to 0.3 if not provided\n",
    "    def featurerank_layer(self, normexprpkl, metadatacsv, frsplit):\n",
    "        norm_express = pd.read_pickle(normexprpkl)\n",
    "        tp = pd.read_csv(metadatacsv, iterator=True, chunksize=1000)\n",
    "        labels = pd.concat(tp, ignore_index=True)\n",
    "        labels.set_index('Unnamed: 0', inplace=True)\n",
    "        labels.index.names = [None]\n",
    "        # Only root level will have no subsetcolumn, all other levels require subsetting\n",
    "        # self.level is one less than what user sees since cellname column is dropped above\n",
    "        metadata_columns = list(labels.columns.values)\n",
    "        labelcolumn = metadata_columns[self.level]\n",
    "        subsetcolumn = metadata_columns[self.level-1] if self.level > 0 else None\n",
    "        subsetcriterium = self.name\n",
    "        # Filter out cells if subsetting necessary, keep only data from given criterium\n",
    "        if subsetcolumn != None:\n",
    "            # Reindex norm_express and labels based on cell names in given criterium\n",
    "            temp = labels.loc[labels[subsetcolumn] == subsetcriterium]\n",
    "            labels = labels.reindex(index=temp.index)\n",
    "            norm_express = norm_express.reindex(index=temp.index)\n",
    "        # Remove cells with labels not provided in the dictionary\n",
    "        if self.labeldict != None:\n",
    "            temp = labels.loc[labels[labelcolumn].isin(list(self.labeldict.values()))]\n",
    "            labels = labels.reindex(index=temp.index)\n",
    "            norm_express = norm_express.reindex(index=temp.index)\n",
    "        norm_express, labels = shuffle(norm_express, labels)\n",
    "        if frsplit is None:\n",
    "            frsplit = 0.3\n",
    "        if frsplit != 1:\n",
    "            norm_express = train_test_split(norm_express, labels, test_size=frsplit, random_state=42, shuffle = True, stratify = labels[labelcolumn])[0]\n",
    "\n",
    "        model = self.xgbmodel\n",
    "        model_bytearray = model.save_raw()[4:]\n",
    "        model.save_raw = lambda: model_bytearray\n",
    "        shap_values = shap.TreeExplainer(model).shap_values(norm_express)\n",
    "        shap.summary_plot(shap_values, norm_express, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path + self.name + '_overallfr.svg')\n",
    "        plt.clf()\n",
    "        for i in range(len(self.labeldict)):\n",
    "            shap.summary_plot(shap_values[i], norm_express, show=False)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(path + self.name + '_class'+str(i)+'fr.svg')\n",
    "            #np.savetxt(path + name + '_class'+str(i)+'fr.csv', shap_values[i], delimiter=\",\")\n",
    "            plt.clf()\n",
    "\n",
    "        vals = np.abs(shap_values).mean(0)\n",
    "        feature_importance = pd.DataFrame(list(zip(norm_express.columns, sum(vals))), columns=['Gene','Feature Importance Value'])\n",
    "        feature_importance.sort_values(by=['Feature Importance Value'], ascending=False, inplace=True)\n",
    "        feature_importance.to_csv(path + self.name + '_featureimportances.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "        print('Overall Feature Ranking: ' + self.name + '_overallfr.svg')\n",
    "        for i in range(len(self.labeldict)):\n",
    "            print(self.name + '--' + list(self.labeldict.values())[i] + ' Feature Ranking: ' + self.name + '_class'+str(i)+'fr.svg')\n",
    "        print('Full Feature Importance List: ' + self.name + '_featureimportances.csv')\n",
    "\n",
    "\n",
    "    # Reads the normalized gene expression data into a norm_expr dataframe\n",
    "    # Reads the specified column in the metadata csv into a label dataframe\n",
    "    # Metadata CSV file must contain samples as row names, first column name (cell A1) is empty\n",
    "    # subsetcolumn and subsetcriterium are strings for extracting rows with the specified label in the specified column\n",
    "    #                                 eg. for per timepoint classification\n",
    "    #                                 can both be set to None if all samples are to be included\n",
    "    # Converts norm_expr and label dataframes into numpy arrays X and Y, splits them into 90/10\n",
    "    # Retrieves cell names of samples in 10% testing data\n",
    "    # global feature_names defined here, list of all gene names -> should be equivalent for all csvs (except subsetted versions)\n",
    "    def read_data(self, normexprpkl, metadatacsv=None, testsplit=None):\n",
    "        norm_express = pd.read_pickle(normexprpkl)\n",
    "        global feature_names\n",
    "        feature_names = list(norm_express)\n",
    "        print(norm_express.shape)\n",
    "\n",
    "        # If validation w/o metadata, metadata not provided, return all data w/o subsetting\n",
    "        if metadatacsv is None:\n",
    "            X = norm_express.values\n",
    "            norm_express.index.name = 'cells'\n",
    "            norm_express.reset_index(inplace=True)\n",
    "            Y = norm_express.values\n",
    "            all_cellnames = Y[:,0]\n",
    "            all_cellnames = all_cellnames.ravel()\n",
    "            Y = None\n",
    "            return X, Y, all_cellnames\n",
    "\n",
    "        tp = pd.read_csv(metadatacsv, iterator=True, chunksize=1000)\n",
    "        labels = pd.concat(tp, ignore_index=True)\n",
    "        labels.set_index('Unnamed: 0', inplace=True)\n",
    "        labels.index.names = [None]\n",
    "\n",
    "        # Only root level will have no subsetcolumn, all other levels require subsetting\n",
    "        # self.level is one less than what user sees since index is dropped above\n",
    "        metadata_columns = list(labels.columns.values)\n",
    "        labelcolumn = metadata_columns[self.level]\n",
    "        subsetcolumn = metadata_columns[self.level-1] if self.level > 0 else None\n",
    "        subsetcriterium = self.name\n",
    "\n",
    "        # Filter out cells if subsetting necessary, keep only data from given criterium\n",
    "        if subsetcolumn != None:\n",
    "            # Reindex norm_express and labels based on cell names in given criterium\n",
    "            temp = labels.loc[labels[subsetcolumn] == subsetcriterium]\n",
    "            labels = labels.reindex(index=temp.index)\n",
    "            norm_express = norm_express.reindex(index=temp.index)\n",
    "        # print (labels[labelcolumn].value_counts())\n",
    "        # Remove cells with labels not provided in the dictionary, replace present keys with values\n",
    "        temp = labels.loc[labels[labelcolumn].isin(list(self.labeldict.values()))]\n",
    "        labels = labels.reindex(index=temp.index)\n",
    "        norm_express = norm_express.reindex(index=temp.index)\n",
    "        print (labels[labelcolumn].value_counts())\n",
    "        for i in range(len(self.labeldict)):\n",
    "            labels = labels.replace(self.labeldict[i],i)\n",
    "\n",
    "        # If validation with metadata, testsplit not provided, return all data w/o train test split\n",
    "        if testsplit is None:\n",
    "            X = norm_express.values\n",
    "            labels.index.name = 'cells'\n",
    "            labels.reset_index(inplace=True)\n",
    "            Y = labels.values\n",
    "            all_cellnames = Y[:,0]\n",
    "            all_cellnames = all_cellnames.ravel()\n",
    "            Y = labels[labelcolumn].values\n",
    "            return X, Y, all_cellnames\n",
    "\n",
    "        X = norm_express.values\n",
    "\n",
    "        # Get cell names in 10% test set - make cell names into a new column to save in a list\n",
    "        labels.index.name = 'cells'\n",
    "        labels.reset_index(inplace=True)\n",
    "        Y = labels.values\n",
    "        np.random.seed(0)\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X_tr, X_test, Y_tr, Y_test = train_test_split(X, Y, test_size=testsplit, random_state=42, shuffle = True, stratify = Y[:,self.level+1])\n",
    "        test_cellnames = Y_test[:,0]\n",
    "        test_cellnames = test_cellnames.ravel()\n",
    "\n",
    "        # Remake train and test split to save X, Y, X_tr, X_test, Y_tr, Y_test\n",
    "        X = norm_express.values\n",
    "        Y = labels[labelcolumn].values\n",
    "        np.random.seed(0)\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X_tr, X_test, Y_tr, Y_test = train_test_split(X, Y, test_size=testsplit, random_state=42, shuffle = True, stratify = Y)\n",
    "        print('Training Samples: ' + str(len(X_tr)) + ', Testing Samples: ' + str(len(X_test)))\n",
    "        return X, Y, X_tr, X_test, Y_tr, Y_test, test_cellnames;\n",
    "\n",
    "\n",
    "    # Calculates metrics of the Layer model on a provided test set and outputs it in a file\n",
    "    # cv_final_val is a naming string to differentiate between cv, final, and validation metrics\n",
    "    def model_metrics(self, cv_final_val, X_test, Y_test):\n",
    "        d_test = xgb.DMatrix(X_test, Y_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        target_names = [str(x) for x in sorted(list(set(Y_test).union(predictions_xgb)))]\n",
    "        metrics = classification_report(Y_test, predictions_xgb, target_names=target_names)\n",
    "        with open(path + self.name + '_' + cv_final_val + 'metrics.txt', 'a+') as f:\n",
    "            print(metrics, file=f)\n",
    "\n",
    "\n",
    "    # Outputs predictions of the Layer model on a provided test set\n",
    "    # Adds a key value pair in the labeldict for an unclassified class, removes it when completed\n",
    "    # 1 csv file outputted for each rejection cutoff in the list rejectioncutoffs\n",
    "    # A cutoff of 0.5 means the probability of the label must be >=0.5 for a prediction to be called\n",
    "    # A cutoff of 0 is equivalent to no rejection option\n",
    "    # train_val is a naming string to differentiate between train/test predictions and validation predictions\n",
    "    def cell_predictions(self, train_val, rejectcutoffs, test_cellnames, X_test, Y_test=None):\n",
    "        d_test = xgb.DMatrix(X_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        self.labeldict[len(self.labeldict)] = 'Unclassified'\n",
    "        for cutoff in rejectcutoffs:\n",
    "            counter = 0\n",
    "            for i in range(len(probabilities_xgb)):\n",
    "                if probabilities_xgb[i,probabilities_xgb.argmax(axis=1)[i]] < cutoff:\n",
    "                    predictions_xgb[i] = len(self.labeldict)-1\n",
    "                    counter += 1\n",
    "            print('Unclassified # of cells w/ probability cutoff=' + str(cutoff) + ': ' + str(counter))\n",
    "            f = open(path + self.name + '_' + train_val + 'predictions_reject' + str(cutoff) + '.csv','w')\n",
    "            if Y_test is not None:\n",
    "                f.write('Cell ID,True Label,Predicted Label')\n",
    "            else:\n",
    "                f.write('Cell ID,Predicted Label')\n",
    "            for j in range(len(self.labeldict)-1):\n",
    "                f.write(',')\n",
    "                f.write(self.labeldict[j] + ' Probability')\n",
    "            f.write('\\n')\n",
    "            for i in range(test_cellnames.size):\n",
    "                f.write(test_cellnames[i])\n",
    "                f.write(',')\n",
    "                if Y_test is not None:\n",
    "                    f.write(self.labeldict[Y_test[i]])\n",
    "                    f.write(',')\n",
    "                f.write(self.labeldict[predictions_xgb[i]])\n",
    "                for j in range(len(self.labeldict)-1):\n",
    "                    f.write(',')\n",
    "                    f.write(str(probabilities_xgb[i][j]))\n",
    "                f.write('\\n')\n",
    "            f.close()\n",
    "        del self.labeldict[len(self.labeldict)-1]\n",
    "\n",
    "\n",
    "    # Outputs a confusion matrix of the Layer model's results on a provided test set\n",
    "    def cfsn_mtx(self, train_val, X_test, Y_test):\n",
    "        d_test = xgb.DMatrix(X_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        predictions_xgb = probabilities_xgb.argmax(axis=1)\n",
    "        classnames = [self.labeldict[x] for x in sorted(list(set(Y_test).union(predictions_xgb)))]\n",
    "\n",
    "        cm = confusion_matrix(Y_test, predictions_xgb)\n",
    "        print(cm)\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classnames))\n",
    "        plt.xticks(tick_marks, classnames, rotation=45)\n",
    "        plt.yticks(tick_marks, classnames)\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, '%0.3f' % cm[i,j] if cm[i,j] > 0 else 0,\n",
    "                horizontalalignment='center', verticalalignment='center',\n",
    "                color='white' if cm[i,j] > thresh else 'black', fontsize=6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path + self.name + '_' + train_val + 'confusionmatrix.svg')\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "    # Creates ROC curves for the Layer model on a provided test set\n",
    "    # Calculates curves using probability values and thresholds\n",
    "    # Outputs 3 figures: micro and macro ROC curves, per class ROC curves, and all combined\n",
    "    def roc_curves(self, X_test, Y_test):\n",
    "        d_test = xgb.DMatrix(X_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        n_classes = len(self.labeldict)\n",
    "        lw = 2\n",
    "\n",
    "        # One hot encode Y_test and predictions arrays\n",
    "        y_test = np.zeros((Y_test.size, Y_test.max()+1))\n",
    "        y_test[np.arange(Y_test.size),Y_test] = 1\n",
    "        y_score = probabilities_xgb\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        # Plot micro/macro ROC curves\n",
    "        plt.figure()\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]),\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]),\n",
    "                 color='navy', linestyle=':', linewidth=4)\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Overall ROC Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(path + self.name + '_miacroroc.svg')\n",
    "        plt.clf()\n",
    "\n",
    "        # Plot class ROC curves\n",
    "        plt.figure()\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], color='#%06X' % random.randint(0, 0xFFFFFF), lw=lw,\n",
    "                     label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                     ''.format(i, roc_auc[i]))\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Class ROC Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(path + self.name + '_classroc.svg')\n",
    "        plt.clf()\n",
    "\n",
    "        # Plot all ROC curves\n",
    "        plt.figure()\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], color='#%06X' % random.randint(0, 0xFFFFFF), lw=lw,\n",
    "                     label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                     ''.format(i, roc_auc[i]))\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]),\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]),\n",
    "                 color='navy', linestyle=':', linewidth=4)\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('Trueas Positive Rate')\n",
    "        plt.title('All ROC Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(path + self.name + '_allroc.svg')\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "    # Creates precision-recall curves for the Layer model on a provided test set\n",
    "    # Calculates curves using probability values and thresholds\n",
    "    # Outputs 1 figure: per class PR curvs and a micro PR curve\n",
    "    def pr_curves(self, X_test, Y_test):\n",
    "        d_test = xgb.DMatrix(X_test, feature_names=feature_names)\n",
    "        probabilities_xgb = self.xgbmodel.predict(d_test)\n",
    "        n_classes = len(self.labeldict)\n",
    "        lw = 2\n",
    "\n",
    "        # One hot encode Y_test and predictions arrays\n",
    "        y_test = np.zeros((Y_test.size, Y_test.max()+1))\n",
    "        y_test[np.arange(Y_test.size),Y_test] = 1\n",
    "        y_score = probabilities_xgb\n",
    "\n",
    "        # Compute precision and recall for each class\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        average_precision = dict()\n",
    "        for i in range(n_classes):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_score[:, i])\n",
    "            average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "\n",
    "        # Compute micro-average precision and recall\n",
    "        precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(), y_score.ravel())\n",
    "        average_precision[\"micro\"] = average_precision_score(y_test, y_score, average=\"micro\")\n",
    "\n",
    "        # Plot micro + class PR curves\n",
    "        plt.figure()\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(recall[i], precision[i], color='#%06X' % random.randint(0, 0xFFFFFF), lw=lw,\n",
    "                     label='PR curve of class {0} (area = {1:0.2f})'\n",
    "                     ''.format(i, average_precision[i]))\n",
    "        plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
    "                 label='micro-average PR curve (area = {0:0.2f})'\n",
    "                       ''.format(average_precision[\"micro\"]),\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('PR Curves')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.savefig(path + self.name + '_allpr.svg')\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function\n",
    "\n",
    "Reads in user input, selects a pathway, and trains / predicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ## CELLPY RUN OPTIONS\n",
    "    # 1a. training w/ cross validation and metrics\n",
    "    #       (runMode = trainOnly, trainNormExpr, labelInfo, trainMetadata, testSplit, rejectionCutoff)\n",
    "    # 1b. training w/o cross validation and metrics\n",
    "    #       (runMode = trainOnly, trainNormExpr, labelInfo, trainMetadata, rejectionCutoff)\n",
    "    # 2a. prediction w/ val_metadata\n",
    "    #       (runMode = predictionOnly, predNormExpr, predMetadata, layerObjectPaths, rejectionCutoff)\n",
    "    # 2b. prediction w/o val_metadata\n",
    "    #       (runMode = predictionOnly, predNormExpr, layerObjectPaths, rejectionCutoff)\n",
    "    # 3.  feature ranking\n",
    "    #       (runMOde = featureRankingOnly, trainNormExpr, trainMetadata, layerObjectPaths, featureRankingSplit)\n",
    "    time_start = time.perf_counter()\n",
    "\n",
    "    # All variables used for training and prediction set to None\n",
    "    global path, rejection_cutoff\n",
    "    path = os.getcwd()\n",
    "    user_train = False\n",
    "    user_predict = False\n",
    "    user_fr = False\n",
    "    train_normexpr = None\n",
    "    labelinfo = None\n",
    "    train_metadata = None\n",
    "    testsplit = None\n",
    "    featrank_on = None\n",
    "    frsplit = None\n",
    "    rejection_cutoff = None\n",
    "    pred_normexpr = None\n",
    "    pred_metadata = None\n",
    "    layer_paths = None\n",
    "\n",
    "    ## Command Line Interface\n",
    "    # runMode must be 'trainOnly', 'predictOnly', or 'featureRankingOnly'\n",
    "    # trainNormExpr, labelInfo, trainMetadata are paths to their respective training files\n",
    "    # testSplit is a float between 0 and 1 denoting the percentage of data to holdout for testing\n",
    "    #           if not provided, cross validation is skipped, 100% model trained w/o metrics\n",
    "    # rejectionCutoff is a float between 0 and 1 denoting the minimum probability for a prediction to not be rejected\n",
    "    # predNormExpr, predMetadata are paths to their respective prediction files\n",
    "    # layerObjectPath is a comma-separated list of paths to the Layer objects that the user wants to predict on the predNormExpr\n",
    "    # featureRankingSplit is a float between 0 and 1 denoting the percentage of data to calculate SHAP importances\n",
    "    args = sys.argv[1:]\n",
    "    options, args = getopt.getopt(args, '',\n",
    "                        ['runMode=', 'trainNormExpr=', 'labelInfo=', 'trainMetadata=', 'testSplit=', 'rejectionCutoff=',\n",
    "                         'predNormExpr=', 'predMetadata=', 'layerObjectPaths=', 'featureRankingSplit='])\n",
    "    for name, value in options:\n",
    "        if name in ['--runMode']:\n",
    "            if value == 'trainOnly':\n",
    "                user_train = True\n",
    "            elif value == 'predictOnly':\n",
    "                user_predict = True\n",
    "            elif value == 'featureRankingOnly':\n",
    "                user_fr = True\n",
    "        if name in ['--trainNormExpr']:\n",
    "            train_normexpr = value\n",
    "        if name in ['--labelInfo']:\n",
    "            labelinfo = value\n",
    "        if name in ['--trainMetadata']:\n",
    "            train_metadata = value\n",
    "        if name in ['--testSplit']:\n",
    "            testsplit = float(value)\n",
    "        if name in ['--rejectionCutoff']:\n",
    "            rejection_cutoff = float(value)\n",
    "        if name in ['--predNormExpr']:\n",
    "            pred_normexpr = value\n",
    "        if name in ['--predMetadata']:\n",
    "            pred_metadata = value\n",
    "        if name in ['--layerObjectPaths']:\n",
    "            layer_paths = value.split(',')\n",
    "        if name in ['--featureRankingSplit']:\n",
    "            frsplit = float(value)\n",
    "\n",
    "    # Check user provided variables follow an above cellpy pathway\n",
    "    passed_options = check_combinations(user_train, user_predict, user_fr, train_normexpr, labelinfo, train_metadata, testsplit,\n",
    "                                        rejection_cutoff, pred_normexpr, pred_metadata, layer_paths, frsplit)\n",
    "    if passed_options is False:\n",
    "        raise ValueError('see printed error log above')\n",
    "\n",
    "    # Create cellpy_results directory with timestamp\n",
    "    newdir = 'cellpy_results_' + datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    path = os.path.join(path, newdir)\n",
    "    if not os.path.isdir(path):\n",
    "        print('Created directory \"cellpy_results\" in cwdir: ' + path)\n",
    "        os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "\n",
    "    # Check training files exist if training option called\n",
    "    passed_train = None\n",
    "    passed_predict = None\n",
    "    passed_fr = None\n",
    "    if user_train is True:\n",
    "        passed_train = check_trainingfiles(train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff)\n",
    "    # Check prediction files exist if prediction option called\n",
    "    if user_predict is True:\n",
    "        passed_predict = check_predictionfiles(pred_normexpr, pred_metadata, layer_paths) \n",
    "    # Check feature ranking files exist if prediction option called\n",
    "    if user_fr is True:\n",
    "        passed_fr = check_featurerankingfiles(train_normexpr, train_metadata, layer_paths, frsplit)\n",
    "    if (passed_train is False) or (passed_predict is False) or (passed_fr is False):\n",
    "        raise ValueError('see printed error log above')\n",
    "\n",
    "    # If training option is called and feasible\n",
    "    if user_train is True and passed_train is True:\n",
    "        training(train_normexpr, labelinfo, train_metadata, testsplit, rejection_cutoff)\n",
    "    # If prediction option is called and feasible\n",
    "    if user_predict is True and passed_predict is True:\n",
    "        prediction(pred_normexpr, pred_metadata, layer_paths)\n",
    "    # If feature ranking option is called and feasible\n",
    "    if user_fr is True and passed_fr is True:\n",
    "        featureranking(train_normexpr, train_metadata, layer_paths, frsplit)\n",
    "\n",
    "    # Print computational time and memory required\n",
    "    time_elapsed = (time.perf_counter() - time_start)\n",
    "    memMb=resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0/1024.0\n",
    "    print (\"%5.1f secs %5.1f MByte\" % (time_elapsed,memMb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
